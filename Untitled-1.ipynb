{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be218b89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RandomForestÔºà0/1/2 ‰∫∫Êï∞Êé®ÂÆöÔºâ\n",
    "# - ÊôÇÁ≥ªÂàósplitÔºàtimestampÈ†ÜÔºöÂâç„Çítrain„ÄÅÂæå„Çç„ÇítestÔºâ\n",
    "# - „Éá„Éº„ÇøÊã°ÂºµÔºàtrain„ÅÆ„ÅøÔºâÔºö„Éé„Ç§„Ç∫ + „Çø„Ç§„É†„Ç∑„Éï„Éà\n",
    "# - ËøΩÂä†ÁâπÂæ¥ÈáèÔºàÊï∞ÂÄ§Á≥ªÔºöCO2/PM2.5/VOC/Ê∏©ÊπøÂ∫¶„Å™„Å©Ôºâ\n",
    "#   window = [5,10,15,20,25,30,60] ÂàÜ„Åß‰∏ÄÊã¨‰ΩúÊàê\n",
    "#   - diff / slope / rolling mean/std / integral(sum)\n",
    "#   - trendÔºàrollingÂõûÂ∏∞ÂÇæ„ÅçÔºâ\n",
    "#   - monotonic_ratioÔºà‰∏äÊòá„Åó„Å¶„ÅÑ„ÇãÂâ≤ÂêàÔºâ\n",
    "#   - accelerationÔºàÂÇæ„Åç„ÅÆÂ§âÂåñÔºâ\n",
    "#   - areaÔºàÂü∫Ê∫ñÂÄ§„Åã„Çâ„ÅÆÁ¥ØÁ©çÈù¢Á©çÔºâ\n",
    "# - ÈÉ®Â±ãÔºöLiving / Japanese\n",
    "# - 3Êù°‰ª∂\n",
    "#   A) allÔºöÂÆ∂Èõª + „Çª„É≥„Çµ„ÉºÔºàM5Stack + PIRÔºâ\n",
    "#   B) appliance_onlyÔºöÂÆ∂Èõª(C0A8033B/C0A80341/C0A80367/C0A80368)„ÅÆ„ÅøÔºàPIR„Å™„ÅóÔºâ\n",
    "#   C) no_co2_humanÔºöCO2„Å®Human(PIR/human)„ÇíÈô§Â§ñÔºàPM2.5/VOC/Ê∏©ÊπøÂ∫¶Á≠â„ÅÆ„ÅøÔºâ\n",
    "# - Âá∫ÂäõÔºöAccuracy + classification_report\n",
    "# - ÊèèÁîªÔºöAccuracyË°®Á§∫„Å®ÂêåÊôÇ„Å´ÔºàÊ∑∑ÂêåË°åÂàó + feature importanceÔºâ„Çí matplotlib „ÅßË°®Á§∫\n",
    "#   ‚Äª‰øùÂ≠ò„Åó„Å™„ÅÑ\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# =========================\n",
    "# Ë®≠ÂÆö\n",
    "# =========================\n",
    "CSV_PATH = \"/mnt/data/smart_home_renamed_with_timestamp.csv\"  # timestamp‰ªò„Åç\n",
    "TIME_COL = \"timestamp\"\n",
    "RESAMPLE_RULE = \"1min\"\n",
    "TRAIN_RATIO = 0.80\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# „Éá„Éº„ÇøÊã°ÂºµÔºàtrain„ÅÆ„ÅøÔºâ\n",
    "AUG_NOISE_LEVEL = 0.02\n",
    "AUG_SHIFT_STEPS = [-1, 1]\n",
    "AUG_REPEAT_NOISE = 1\n",
    "\n",
    "# ÁâπÂæ¥Èáè window\n",
    "WINS = [5, 10, 15, 20, 25, 30, 60]\n",
    "\n",
    "# importanceË°®Á§∫„ÅØTopKÔºàÂÖ®‰ª∂ÊèèÁîª„ÅØË¶ã„Å´„Åè„ÅÑ„ÅÆ„ÅßTop„Å†„ÅëÔºâ\n",
    "TOPK_IMPORTANCE = 15\n",
    "\n",
    "# ÁõÆÁöÑÂ§âÊï∞Ôºà0/1/2„ÅÆ„ÅøÔºâ\n",
    "ALLOWED_Y = [0, 1, 2]\n",
    "\n",
    "# ÂÆ∂Èõªprefix\n",
    "APPLIANCE_PREFIXES = (\"C0A8033B\", \"C0A80341\", \"C0A80367\", \"C0A80368\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utility\n",
    "# =========================\n",
    "def to_dt(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(series, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "def safe_ffill(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.ffill().infer_objects(copy=False)\n",
    "\n",
    "def resample_1min(df: pd.DataFrame, rule=\"1min\") -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    other_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "    out = []\n",
    "    if num_cols:\n",
    "        out.append(df[num_cols].resample(rule).mean())\n",
    "    if other_cols:\n",
    "        out.append(df[other_cols].resample(rule).last())\n",
    "\n",
    "    dfr = pd.concat(out, axis=1).sort_index()\n",
    "    dfr = safe_ffill(dfr)\n",
    "    return dfr\n",
    "\n",
    "def time_split_idx(n: int, train_ratio: float):\n",
    "    n_tr = int(n * train_ratio)\n",
    "    tr_idx = np.arange(0, n_tr)\n",
    "    te_idx = np.arange(n_tr, n)\n",
    "    return tr_idx, te_idx\n",
    "\n",
    "def coerce_boolish_to_float(s: pd.Series) -> pd.Series:\n",
    "    s = s.replace({\"True\": 1, \"False\": 0, True: 1, False: 0})\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s.astype(float)\n",
    "\n",
    "def uniq(xs):\n",
    "    return list(dict.fromkeys(xs))\n",
    "\n",
    "def plot_cm_and_importance(y_true, y_pred, labels, feature_names, importances, title_prefix, topk=15):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # TopK importance\n",
    "    imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    top = imp.head(topk)[::-1]\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Left: confusion matrix\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    im = ax1.imshow(cm, interpolation=\"nearest\")\n",
    "    ax1.set_title(f\"{title_prefix}\\nAccuracy={acc:.4f}\")\n",
    "    plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "\n",
    "    tick = np.arange(len(labels))\n",
    "    ax1.set_xticks(tick)\n",
    "    ax1.set_yticks(tick)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.set_yticklabels(labels)\n",
    "    ax1.set_xlabel(\"Predicted\")\n",
    "    ax1.set_ylabel(\"True\")\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax1.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Right: feature importance\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.barh(top.index, top.values)\n",
    "    ax2.set_title(f\"Feature Importance (Top{topk})\")\n",
    "    ax2.set_xlabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature EngineeringÔºàÊï∞ÂÄ§Á≥ªÔºâ\n",
    "# =========================\n",
    "def _rolling_linreg_slope(y: pd.Series, window: int) -> pd.Series:\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    w = int(window)\n",
    "    if w < 3:\n",
    "        return pd.Series(0.0, index=y.index)\n",
    "\n",
    "    t = np.arange(w, dtype=float)\n",
    "    t_centered = t - t.mean()\n",
    "    denom = np.sum(t_centered ** 2)\n",
    "\n",
    "    def slope_fn(a):\n",
    "        a = np.asarray(a, dtype=float)\n",
    "        a_centered = a - a.mean()\n",
    "        return np.dot(t_centered, a_centered) / denom\n",
    "\n",
    "    return y.rolling(w, min_periods=w).apply(slope_fn, raw=True).fillna(0.0)\n",
    "\n",
    "def _monotonic_ratio(y: pd.Series, window: int) -> pd.Series:\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    d = y.diff()\n",
    "    pos = (d > 0).astype(float)\n",
    "    return pos.rolling(window, min_periods=1).mean().fillna(0.0)\n",
    "\n",
    "def _area_from_baseline(y: pd.Series, window: int) -> pd.Series:\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    base = y.rolling(window, min_periods=1).min()\n",
    "    return (y - base).rolling(window, min_periods=1).sum().fillna(0.0)\n",
    "\n",
    "def build_numeric_features_multiwin(df: pd.DataFrame, cols: list[str], wins: list[int]) -> pd.DataFrame:\n",
    "    feats = {}\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\").astype(float).ffill()\n",
    "\n",
    "        feats[c] = s\n",
    "        d1 = s.diff().fillna(0.0)\n",
    "        feats[f\"{c}_diff_1min\"] = d1\n",
    "\n",
    "        for w in wins:\n",
    "            feats[f\"{c}_mean_{w}min\"] = s.rolling(w, min_periods=1).mean().fillna(0.0)\n",
    "            feats[f\"{c}_std_{w}min\"]  = s.rolling(w, min_periods=1).std().fillna(0.0)\n",
    "            feats[f\"{c}_slope_{w}min\"] = ((s - s.shift(w)) / float(w)).fillna(0.0)\n",
    "            feats[f\"{c}_integral_{w}min\"] = s.rolling(w, min_periods=1).sum().fillna(0.0)\n",
    "\n",
    "            feats[f\"{c}_trend_{w}min\"] = _rolling_linreg_slope(s, w)\n",
    "            feats[f\"{c}_monotonic_ratio_{w}min\"] = _monotonic_ratio(s, w)\n",
    "            feats[f\"{c}_area_{w}min\"] = _area_from_baseline(s, w)\n",
    "\n",
    "            feats[f\"{c}_accel_{w}min\"] = (d1 - d1.rolling(w, min_periods=1).mean()).fillna(0.0)\n",
    "\n",
    "    X = pd.DataFrame(feats, index=df.index)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "    return X\n",
    "\n",
    "def build_human_features(df: pd.DataFrame, human_cols: list[str], wins: list[int]) -> pd.DataFrame:\n",
    "    feats = {}\n",
    "    for c in human_cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = coerce_boolish_to_float(df[c]).ffill().fillna(0.0)\n",
    "        feats[c] = s\n",
    "        for w in wins:\n",
    "            feats[f\"{c}_sum_{w}min\"] = s.rolling(w, min_periods=1).sum().fillna(0.0)\n",
    "\n",
    "    X = pd.DataFrame(feats, index=df.index)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Column pickerÔºàÈÉ®Â±ã„Åî„Å®Ôºâ\n",
    "# =========================\n",
    "def is_pm25(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    return (\"pm2_5\" in n) or (\"pm2.5\" in n) or (\"pm25\" in n)\n",
    "\n",
    "def is_co2(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    return n.endswith(\"_co2\") or (\"_co2_\" in n)\n",
    "\n",
    "def is_voc(name: str) -> bool:\n",
    "    return \"voc\" in name.lower()\n",
    "\n",
    "def is_temp_hum_like(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    return (\"_temp\" in n) or (\"temp\" in n) or (\"_hum\" in n) or (\"humid\" in n)\n",
    "\n",
    "def pick_room_columns(df: pd.DataFrame, room: str):\n",
    "    if room == \"Living\":\n",
    "        label = \"Label_Living_Count\"\n",
    "        appliance_cols = [c for c in df.columns if c.startswith(\"C0A80367-013001_\") or c.startswith(\"C0A8033B-013501_\")]\n",
    "    elif room == \"Japanese\":\n",
    "        label = \"Label_Japanese_Count\"\n",
    "        appliance_cols = [c for c in df.columns if c.startswith(\"C0A80368-013001_\") or c.startswith(\"C0A80341-013501_\")]\n",
    "    else:\n",
    "        raise ValueError(\"room must be Living or Japanese\")\n",
    "\n",
    "    # M5Stack: 1/2/8\n",
    "    m5_cols = [c for c in df.columns if (c.startswith(\"M5Stack1_\") or c.startswith(\"M5Stack2_\") or c.startswith(\"M5Stack8_\"))]\n",
    "    pir_cols = [c for c in df.columns if c.lower().startswith(\"pir\")]\n",
    "\n",
    "    candidate_num = appliance_cols + m5_cols\n",
    "\n",
    "    co2_cols  = [c for c in candidate_num if is_co2(c)]\n",
    "    pm25_cols = [c for c in candidate_num if is_pm25(c)]\n",
    "    voc_cols  = [c for c in candidate_num if is_voc(c)]\n",
    "    th_cols   = [c for c in candidate_num if is_temp_hum_like(c)]\n",
    "\n",
    "    human_cols = [c for c in candidate_num if c.lower().endswith(\"_human\")] + pir_cols\n",
    "\n",
    "    appliance_only = [c for c in df.columns if c.startswith(APPLIANCE_PREFIXES)]\n",
    "    app_co2  = [c for c in appliance_only if is_co2(c)]\n",
    "    app_pm25 = [c for c in appliance_only if is_pm25(c)]\n",
    "    app_voc  = [c for c in appliance_only if is_voc(c)]\n",
    "    app_th   = [c for c in appliance_only if is_temp_hum_like(c)]\n",
    "    app_human= [c for c in appliance_only if c.lower().endswith(\"_human\")]\n",
    "\n",
    "    return dict(\n",
    "        label=label,\n",
    "        co2_cols=uniq([c for c in co2_cols if c in df.columns]),\n",
    "        pm25_cols=uniq([c for c in pm25_cols if c in df.columns]),\n",
    "        voc_cols=uniq([c for c in voc_cols if c in df.columns]),\n",
    "        th_cols=uniq([c for c in th_cols if c in df.columns]),\n",
    "        human_cols=uniq([c for c in human_cols if c in df.columns]),\n",
    "        app_co2=uniq([c for c in app_co2 if c in df.columns]),\n",
    "        app_pm25=uniq([c for c in app_pm25 if c in df.columns]),\n",
    "        app_voc=uniq([c for c in app_voc if c in df.columns]),\n",
    "        app_th=uniq([c for c in app_th if c in df.columns]),\n",
    "        app_human=uniq([c for c in app_human if c in df.columns]),\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data augmentationÔºàtrain„ÅÆ„ÅøÔºâ\n",
    "# =========================\n",
    "def augment_data(X: pd.DataFrame, y: pd.Series, noise_level=0.02, shift_steps=(-1, 1), repeat_noise=1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    X_list = [X]\n",
    "    y_list = [y.reset_index(drop=True)]\n",
    "\n",
    "    # Êï∞ÂÄ§Âàó„ÇíÁ¢∫ÂÆöÔºà„Ç∫„É¨Èò≤Ê≠¢Ôºâ\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X_num = X[num_cols].to_numpy()  # shapeÂõ∫ÂÆö\n",
    "\n",
    "    # 1) noise\n",
    "    for _ in range(int(repeat_noise)):\n",
    "        if len(num_cols) > 0:\n",
    "            noise = rng.normal(0, noise_level, size=X_num.shape)\n",
    "            Xn = X.copy()\n",
    "            Xn.loc[:, num_cols] = X_num + noise\n",
    "            X_list.append(Xn)\n",
    "            y_list.append(y.reset_index(drop=True))\n",
    "\n",
    "    # 2) shift\n",
    "    for step in shift_steps:\n",
    "        Xs = X.shift(step).bfill().ffill()\n",
    "        X_list.append(Xs)\n",
    "        y_list.append(y.reset_index(drop=True))\n",
    "\n",
    "    X_aug = pd.concat(X_list, axis=0, ignore_index=True)\n",
    "    y_aug = pd.concat(y_list, axis=0, ignore_index=True)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Train / EvalÔºàRFÔºâ\n",
    "# =========================\n",
    "def run_rf_one_setting(room: str, setting: str, df: pd.DataFrame, pack: dict):\n",
    "    label_col = pack[\"label\"]\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[SKIP] {room}: {label_col} not found\")\n",
    "        return\n",
    "\n",
    "    # y: 0/1/2 only\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\")\n",
    "    y = y.where(y.isin(ALLOWED_Y), np.nan).ffill().fillna(0).astype(int)\n",
    "\n",
    "    # select columns by setting\n",
    "    if setting == \"all\":\n",
    "        num_cols = pack[\"co2_cols\"] + pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        human_cols = pack[\"human_cols\"]\n",
    "    elif setting == \"appliance_only\":\n",
    "        num_cols = pack[\"app_co2\"] + pack[\"app_pm25\"] + pack[\"app_voc\"] + pack[\"app_th\"]\n",
    "        human_cols = pack[\"app_human\"]  # PIR„Å™„Åó\n",
    "    elif setting == \"no_co2_human\":\n",
    "        # CO2„Å®human„ÇíÂ§ñ„Åô\n",
    "        num_cols = pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        human_cols = []\n",
    "    else:\n",
    "        raise ValueError(\"invalid setting\")\n",
    "\n",
    "    num_cols = uniq([c for c in num_cols if c in df.columns])\n",
    "    human_cols = uniq([c for c in human_cols if c in df.columns])\n",
    "\n",
    "    # build features\n",
    "    X_num = build_numeric_features_multiwin(df, num_cols, WINS) if len(num_cols) else pd.DataFrame(index=df.index)\n",
    "    X_hum = build_human_features(df, human_cols, WINS) if len(human_cols) else pd.DataFrame(index=df.index)\n",
    "    X = pd.concat([X_num, X_hum], axis=1).replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "    # time split\n",
    "    tr_idx, te_idx = time_split_idx(len(df), TRAIN_RATIO)\n",
    "    X_train = X.iloc[tr_idx].reset_index(drop=True)\n",
    "    y_train = y.iloc[tr_idx].reset_index(drop=True)\n",
    "    X_test  = X.iloc[te_idx].reset_index(drop=True)\n",
    "    y_test  = y.iloc[te_idx].reset_index(drop=True)\n",
    "\n",
    "    # augmentation (train only)\n",
    "    print(f\"[{room} / {setting}] „Éá„Éº„ÇøÊã°Âºµ‰∏≠...\")\n",
    "    X_train_aug, y_train_aug = augment_data(\n",
    "        X_train, y_train,\n",
    "        noise_level=AUG_NOISE_LEVEL,\n",
    "        shift_steps=AUG_SHIFT_STEPS,\n",
    "        repeat_noise=AUG_REPEAT_NOISE,\n",
    "        seed=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"[{room} / {setting}] Â≠¶Áøí„Éá„Éº„ÇøÊï∞: {len(X_train_aug)}\")\n",
    "\n",
    "    # train RF\n",
    "    print(f\"[{room} / {setting}] „É¢„Éá„É´Â≠¶Áøí‰∏≠...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"  # „ÅÇ„Å™„Åü„ÅÆÂÖÉ„Ç≥„Éº„Éâ„Å´Âêà„Çè„Åõ„Çã\n",
    "    )\n",
    "    model.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "    # eval\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚òÖ {room} / {setting} Ê≠£Ëß£Áéá (Accuracy): {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    # plot (CM + importance) --- ‰øùÂ≠ò„Åó„Å™„ÅÑ\n",
    "    importances = model.feature_importances_\n",
    "    plot_cm_and_importance(\n",
    "        y_true=y_test, y_pred=y_pred,\n",
    "        labels=[0, 1, 2],\n",
    "        feature_names=X.columns.tolist(),\n",
    "        importances=importances,\n",
    "        title_prefix=f\"RF {room} / {setting}\",\n",
    "        topk=TOPK_IMPORTANCE\n",
    "    )\n",
    "\n",
    "\n",
    "def run_room(df: pd.DataFrame, room: str):\n",
    "    pack = pick_room_columns(df, room)\n",
    "    for setting in [\"all\", \"appliance_only\", \"no_co2_human\"]:\n",
    "        run_rf_one_setting(room, setting, df, pack)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "print(\"üì• Loading CSV...\")\n",
    "full_data = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "# timestamp sort\n",
    "if TIME_COL in full_data.columns:\n",
    "    full_data[TIME_COL] = to_dt(full_data[TIME_COL])\n",
    "    full_data = full_data.dropna(subset=[TIME_COL]).sort_values(TIME_COL).reset_index(drop=True)\n",
    "else:\n",
    "    raise ValueError(f\"{TIME_COL} Âàó„Åå„ÅÇ„Çä„Åæ„Åõ„ÇìÔºàtimestamp‰ªò„ÅçCSV„Çí‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºâ\")\n",
    "\n",
    "# forward fill (Êú™Êù•„ÇíË¶ã„Å™„ÅÑÔºö„Åì„ÅÆÊÆµÈöé„ÅØË°åÈ†Ü„ÅÆffill„Å™„ÅÆ„ÅßOK)\n",
    "full_data = safe_ffill(full_data)\n",
    "full_data.fillna(0, inplace=True)\n",
    "\n",
    "# index„Çítimestamp„Å´„Åó„Å¶resample\n",
    "full_data = full_data.set_index(TIME_COL)\n",
    "df = resample_1min(full_data, RESAMPLE_RULE)\n",
    "\n",
    "print(\"\\n================================================================================\")\n",
    "print(\"RUN ROOM = Living\")\n",
    "print(\"================================================================================\")\n",
    "run_room(df, \"Living\")\n",
    "\n",
    "print(\"\\n================================================================================\")\n",
    "print(\"RUN ROOM = Japanese\")\n",
    "print(\"================================================================================\")\n",
    "run_room(df, \"Japanese\")\n",
    "\n",
    "print(\"\\n‚úÖ Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
