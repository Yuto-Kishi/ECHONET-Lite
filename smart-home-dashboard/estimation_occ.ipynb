{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deea7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/8k_9t5_n3sng20r8pshmdzmr0000gn/T/ipykernel_20889/3407695055.py:98: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.bfill().ffill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Living: drop constant cols (7): ['C0A8033B-013501_opStatus', 'C0A8033B-013501_flow', 'C0A80367-013001_mode', 'C0A80367-013001_blowTemp', 'C0A80367-013001_flow', 'C0A80367-013001_blowTemp_rate', 'C0A80367-013001_blowTemp_rate_smooth']\n",
      "\n",
      "==========================================================================================\n",
      "[ROOM] Living\n",
      "[INFO] label=Label_Living_Count | N=13606 | train=9524 val=2041 test=2041\n",
      "[INFO] features=72 (show first 15):\n",
      "  PIR1_motion, PIR2_motion, PIR3_motion, PIR4_motion, PIR18_motion, PIR13_motion, PIR11_motion, PIR21_motion, PIR17_motion, PIR10_motion, PIR15_motion, PIR19_motion, PIR20_motion, PIR22_motion, PIR24_motion ...\n",
      "==========================================================================================\n",
      "\n",
      "[DEBUG][Living][Occupancy(train-slice)] NaN top5:\n",
      "PIR1_motion                 0\n",
      "PIR2_motion                 0\n",
      "M5Stack8_co2_rate           0\n",
      "M5Stack2_voc_rate_smooth    0\n",
      "M5Stack2_voc_rate           0\n",
      "[DEBUG][Living][Occupancy(train-slice)] inf top5:\n",
      "PIR1_motion                 0\n",
      "PIR2_motion                 0\n",
      "M5Stack8_co2_rate           0\n",
      "M5Stack2_voc_rate_smooth    0\n",
      "M5Stack2_voc_rate           0\n",
      "\n",
      "[Living-Occupancy] optimizer=AdamW lr=0.001 weight_decay=0.0001\n",
      "[Living-Occupancy] loss=CrossEntropyLoss\n",
      "[Living-Occupancy][Epoch 01] train_loss=0.3335 val_loss=0.8287 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 02] train_loss=0.0938 val_loss=1.1002 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 03] train_loss=0.0569 val_loss=1.2513 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 04] train_loss=0.0395 val_loss=1.5040 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 05] train_loss=0.0312 val_loss=1.6208 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 06] train_loss=0.0257 val_loss=1.4627 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 07] train_loss=0.0218 val_loss=1.8964 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 08] train_loss=0.0184 val_loss=1.5917 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 09] train_loss=0.0159 val_loss=1.9932 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 10] train_loss=0.0174 val_loss=2.5122 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 11] train_loss=0.0131 val_loss=2.4460 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 12] train_loss=0.0113 val_loss=2.7716 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 13] train_loss=0.0097 val_loss=2.9973 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 14] train_loss=0.0101 val_loss=2.6131 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 15] train_loss=0.0102 val_loss=2.8166 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 16] train_loss=0.0079 val_loss=3.0167 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 17] train_loss=0.0084 val_loss=3.7265 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 18] train_loss=0.0083 val_loss=3.3963 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 19] train_loss=0.0069 val_loss=3.5110 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 20] train_loss=0.0058 val_loss=3.7802 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 21] train_loss=0.0074 val_loss=3.4508 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 22] train_loss=0.0058 val_loss=3.6066 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 23] train_loss=0.0101 val_loss=4.0396 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 24] train_loss=0.0059 val_loss=3.4697 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 25] train_loss=0.0056 val_loss=4.5476 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 26] train_loss=0.0036 val_loss=4.5828 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 27] train_loss=0.0040 val_loss=4.4547 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 28] train_loss=0.0029 val_loss=4.6837 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 29] train_loss=0.0022 val_loss=4.6258 (lr=1.00e-03)\n",
      "[Living-Occupancy][Epoch 30] train_loss=0.0027 val_loss=4.5532 (lr=1.00e-03)\n",
      "\n",
      "[Living][TEST] Occupancy (0/1) acc = 0.7893\n",
      "Confusion matrix:\n",
      " [[1083    2]\n",
      " [ 428  528]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.83      1085\n",
      "           1       1.00      0.55      0.71       956\n",
      "\n",
      "    accuracy                           0.79      2041\n",
      "   macro avg       0.86      0.78      0.77      2041\n",
      "weighted avg       0.85      0.79      0.78      2041\n",
      "\n",
      "\n",
      "[Living] Occupancy Permutation Importance (Top 15) | base_acc=0.7893\n",
      "                 feature  importance_drop_acc\n",
      "   C0A80367-013001_human             0.029234\n",
      "             PIR4_motion             0.024988\n",
      "            M5Stack8_co2             0.015025\n",
      "            M5Stack8_hum             0.014372\n",
      "            M5Stack2_voc             0.014209\n",
      "M5Stack8_co2_rate_smooth             0.013882\n",
      "     C0A80367-013001_co2             0.013719\n",
      "            M5Stack2_co2             0.012902\n",
      "       M5Stack8_co2_rate             0.009472\n",
      "M5Stack2_co2_rate_smooth             0.008166\n",
      "       M5Stack8_voc_rate             0.006369\n",
      "            M5Stack8_voc             0.005390\n",
      "       M5Stack2_voc_rate             0.005063\n",
      "M5Stack2_voc_rate_smooth             0.004410\n",
      "            M5Stack2_hum             0.003756\n",
      "\n",
      "[DEBUG][Living][Count(train-slice)] NaN top5:\n",
      "PIR1_motion                 0\n",
      "PIR2_motion                 0\n",
      "M5Stack8_co2_rate           0\n",
      "M5Stack2_voc_rate_smooth    0\n",
      "M5Stack2_voc_rate           0\n",
      "[DEBUG][Living][Count(train-slice)] inf top5:\n",
      "PIR1_motion                 0\n",
      "PIR2_motion                 0\n",
      "M5Stack8_co2_rate           0\n",
      "M5Stack2_voc_rate_smooth    0\n",
      "M5Stack2_voc_rate           0\n",
      "\n",
      "[Living-Count(0/1/2)] optimizer=AdamW lr=0.001 weight_decay=0.0001\n",
      "[Living-Count(0/1/2)] loss=CrossEntropyLoss\n",
      "[Living-Count(0/1/2)][Epoch 01] train_loss=0.5950 val_loss=1.3441 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 02] train_loss=0.2343 val_loss=1.7118 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 03] train_loss=0.1476 val_loss=1.6728 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 04] train_loss=0.1038 val_loss=1.6482 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 05] train_loss=0.0766 val_loss=1.9638 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 06] train_loss=0.0593 val_loss=1.9992 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 07] train_loss=0.0528 val_loss=1.9569 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 08] train_loss=0.0479 val_loss=2.2207 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 09] train_loss=0.0387 val_loss=2.2858 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 10] train_loss=0.0374 val_loss=2.5016 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 11] train_loss=0.0304 val_loss=2.5275 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 12] train_loss=0.0298 val_loss=2.8034 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 13] train_loss=0.0265 val_loss=2.9760 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 14] train_loss=0.0248 val_loss=2.9441 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 15] train_loss=0.0204 val_loss=3.2481 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 16] train_loss=0.0211 val_loss=4.1366 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 17] train_loss=0.0199 val_loss=4.0263 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 18] train_loss=0.0163 val_loss=3.9096 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 19] train_loss=0.0164 val_loss=3.8550 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 20] train_loss=0.0158 val_loss=3.5295 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 21] train_loss=0.0152 val_loss=3.8332 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 22] train_loss=0.0128 val_loss=4.4482 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 23] train_loss=0.0117 val_loss=3.8938 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 24] train_loss=0.0135 val_loss=3.9330 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 25] train_loss=0.0126 val_loss=4.0873 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 26] train_loss=0.0134 val_loss=3.9891 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 27] train_loss=0.0109 val_loss=4.2300 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 28] train_loss=0.0109 val_loss=4.5796 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 29] train_loss=0.0091 val_loss=5.2827 (lr=1.00e-03)\n",
      "[Living-Count(0/1/2)][Epoch 30] train_loss=0.0071 val_loss=5.1380 (lr=1.00e-03)\n",
      "\n",
      "[Living][TEST] Count (0/1/2) acc = 0.6193\n",
      "Confusion matrix:\n",
      " [[1083    0    2]\n",
      " [ 377   26  391]\n",
      " [   7    0  155]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85      1085\n",
      "           1       1.00      0.03      0.06       794\n",
      "           2       0.28      0.96      0.44       162\n",
      "\n",
      "    accuracy                           0.62      2041\n",
      "   macro avg       0.67      0.66      0.45      2041\n",
      "weighted avg       0.80      0.62      0.51      2041\n",
      "\n",
      "\n",
      "[Living] Count Permutation Importance (Top 15) | base_acc=0.6193\n",
      "                        feature  importance_drop_acc\n",
      "                   M5Stack8_voc             0.005879\n",
      "                   M5Stack8_co2             0.005879\n",
      "            C0A80367-013001_co2             0.004083\n",
      "                 M5Stack8_pm2_5             0.003266\n",
      "                  M5Stack2_temp             0.003103\n",
      "          C0A80367-013001_power             0.002940\n",
      "           C0A8033B-013501_temp             0.002613\n",
      "       C0A80367-013001_roomTemp             0.002450\n",
      "                   M5Stack2_voc             0.002286\n",
      "                   M5Stack2_co2             0.001470\n",
      "                   M5Stack2_hum             0.001307\n",
      "C0A8033B-013501_hum_rate_smooth             0.000980\n",
      "       C0A8033B-013501_hum_rate             0.000980\n",
      "          C0A8033B-013501_power             0.000980\n",
      "      M5Stack2_temp_rate_smooth             0.000653\n",
      "[INFO] Japanese: drop constant cols (7): ['C0A80341-013501_opStatus', 'C0A80341-013501_flow', 'C0A80368-013001_mode', 'C0A80368-013001_blowTemp', 'C0A80368-013001_flow', 'C0A80368-013001_blowTemp_rate', 'C0A80368-013001_blowTemp_rate_smooth']\n",
      "\n",
      "==========================================================================================\n",
      "[ROOM] Japanese\n",
      "[INFO] label=Label_Japanese_Count | N=13606 | train=9524 val=2041 test=2041\n",
      "[INFO] features=59 (show first 15):\n",
      "  PIR21_motion, PIR17_motion, M5Stack1_co2, M5Stack1_temp, M5Stack1_hum, M5Stack1_pm2_5, M5Stack1_voc, M5Stack10_co2, M5Stack10_temp, M5Stack10_hum, M5Stack10_pm2_5, M5Stack10_voc, C0A80341-013501_temp, C0A80341-013501_hum, C0A80341-013501_pm25 ...\n",
      "==========================================================================================\n",
      "\n",
      "[DEBUG][Japanese][Occupancy(train-slice)] NaN top5:\n",
      "PIR21_motion                 0\n",
      "M5Stack10_hum_rate_smooth    0\n",
      "M5Stack1_co2_rate_smooth     0\n",
      "M5Stack1_temp_rate           0\n",
      "M5Stack1_temp_rate_smooth    0\n",
      "[DEBUG][Japanese][Occupancy(train-slice)] inf top5:\n",
      "PIR21_motion                 0\n",
      "M5Stack10_hum_rate_smooth    0\n",
      "M5Stack1_co2_rate_smooth     0\n",
      "M5Stack1_temp_rate           0\n",
      "M5Stack1_temp_rate_smooth    0\n",
      "\n",
      "[Japanese-Occupancy] optimizer=AdamW lr=0.001 weight_decay=0.0001\n",
      "[Japanese-Occupancy] loss=CrossEntropyLoss\n",
      "[Japanese-Occupancy][Epoch 01] train_loss=0.3175 val_loss=0.2153 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 02] train_loss=0.0864 val_loss=0.1664 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 03] train_loss=0.0594 val_loss=0.1438 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 04] train_loss=0.0472 val_loss=0.1485 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 05] train_loss=0.0404 val_loss=0.1626 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 06] train_loss=0.0357 val_loss=0.1703 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 07] train_loss=0.0312 val_loss=0.1920 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 08] train_loss=0.0271 val_loss=0.2213 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 09] train_loss=0.0231 val_loss=0.2454 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 10] train_loss=0.0227 val_loss=0.2796 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 11] train_loss=0.0213 val_loss=0.3083 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 12] train_loss=0.0171 val_loss=0.3331 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 13] train_loss=0.0179 val_loss=0.3171 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 14] train_loss=0.0148 val_loss=0.3215 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 15] train_loss=0.0144 val_loss=0.4063 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 16] train_loss=0.0121 val_loss=0.4023 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 17] train_loss=0.0137 val_loss=0.3505 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 18] train_loss=0.0112 val_loss=0.4435 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 19] train_loss=0.0115 val_loss=0.4401 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 20] train_loss=0.0106 val_loss=0.4514 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 21] train_loss=0.0092 val_loss=0.4448 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 22] train_loss=0.0098 val_loss=0.4888 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 23] train_loss=0.0115 val_loss=0.4430 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 24] train_loss=0.0089 val_loss=0.4566 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 25] train_loss=0.0075 val_loss=0.5539 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 26] train_loss=0.0068 val_loss=0.5672 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 27] train_loss=0.0073 val_loss=0.5917 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 28] train_loss=0.0060 val_loss=0.5876 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 29] train_loss=0.0059 val_loss=0.5566 (lr=1.00e-03)\n",
      "[Japanese-Occupancy][Epoch 30] train_loss=0.0062 val_loss=0.6433 (lr=1.00e-03)\n",
      "\n",
      "[Japanese][TEST] Occupancy (0/1) acc = 0.9515\n",
      "Confusion matrix:\n",
      " [[1195   76]\n",
      " [  23  747]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1271\n",
      "           1       0.91      0.97      0.94       770\n",
      "\n",
      "    accuracy                           0.95      2041\n",
      "   macro avg       0.94      0.96      0.95      2041\n",
      "weighted avg       0.95      0.95      0.95      2041\n",
      "\n",
      "\n",
      "[Japanese] Occupancy Permutation Importance (Top 15) | base_acc=0.9515\n",
      "                        feature  importance_drop_acc\n",
      "       M5Stack1_co2_rate_smooth             0.016985\n",
      "                M5Stack10_pm2_5             0.008493\n",
      "            C0A80341-013501_gas             0.007186\n",
      "                  M5Stack10_co2             0.006043\n",
      "C0A80368-013001_co2_rate_smooth             0.005226\n",
      "              M5Stack1_co2_rate             0.003920\n",
      "       M5Stack1_voc_rate_smooth             0.003266\n",
      "            C0A80368-013001_co2             0.002613\n",
      "C0A80368-013001_hum_rate_smooth             0.002450\n",
      "              M5Stack1_voc_rate             0.002123\n",
      "                   M5Stack1_co2             0.001960\n",
      "      M5Stack10_co2_rate_smooth             0.001470\n",
      "      M5Stack1_temp_rate_smooth             0.001143\n",
      "                 M5Stack1_pm2_5             0.000980\n",
      "             M5Stack10_hum_rate             0.000980\n",
      "\n",
      "[DEBUG][Japanese][Count(train-slice)] NaN top5:\n",
      "PIR21_motion                 0\n",
      "M5Stack10_hum_rate_smooth    0\n",
      "M5Stack1_co2_rate_smooth     0\n",
      "M5Stack1_temp_rate           0\n",
      "M5Stack1_temp_rate_smooth    0\n",
      "[DEBUG][Japanese][Count(train-slice)] inf top5:\n",
      "PIR21_motion                 0\n",
      "M5Stack10_hum_rate_smooth    0\n",
      "M5Stack1_co2_rate_smooth     0\n",
      "M5Stack1_temp_rate           0\n",
      "M5Stack1_temp_rate_smooth    0\n",
      "\n",
      "[Japanese-Count(0/1/2)] optimizer=AdamW lr=0.001 weight_decay=0.0001\n",
      "[Japanese-Count(0/1/2)] loss=CrossEntropyLoss\n",
      "[Japanese-Count(0/1/2)][Epoch 01] train_loss=0.6074 val_loss=0.5146 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 02] train_loss=0.2227 val_loss=0.4777 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 03] train_loss=0.1231 val_loss=0.4262 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 04] train_loss=0.0830 val_loss=0.4474 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 05] train_loss=0.0661 val_loss=0.3709 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 06] train_loss=0.0516 val_loss=0.3504 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 07] train_loss=0.0466 val_loss=0.3794 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 08] train_loss=0.0398 val_loss=0.3794 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 09] train_loss=0.0311 val_loss=0.4271 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 10] train_loss=0.0301 val_loss=0.4167 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 11] train_loss=0.0271 val_loss=0.4601 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 12] train_loss=0.0272 val_loss=0.4377 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 13] train_loss=0.0242 val_loss=0.5118 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 14] train_loss=0.0244 val_loss=0.5167 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 15] train_loss=0.0209 val_loss=0.5220 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 16] train_loss=0.0184 val_loss=0.4985 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 17] train_loss=0.0195 val_loss=0.4983 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 18] train_loss=0.0154 val_loss=0.5169 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 19] train_loss=0.0160 val_loss=0.5235 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 20] train_loss=0.0150 val_loss=0.4732 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 21] train_loss=0.0123 val_loss=0.4989 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 22] train_loss=0.0148 val_loss=0.5443 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 23] train_loss=0.0119 val_loss=0.5470 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 24] train_loss=0.0124 val_loss=0.6254 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 25] train_loss=0.0098 val_loss=0.5512 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 26] train_loss=0.0113 val_loss=0.5019 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 27] train_loss=0.0104 val_loss=0.5166 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 28] train_loss=0.0086 val_loss=0.6317 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 29] train_loss=0.0098 val_loss=0.5421 (lr=1.00e-03)\n",
      "[Japanese-Count(0/1/2)][Epoch 30] train_loss=0.0095 val_loss=0.5451 (lr=1.00e-03)\n",
      "\n",
      "[Japanese][TEST] Count (0/1/2) acc = 0.8775\n",
      "Confusion matrix:\n",
      " [[1220   31   20]\n",
      " [  32  425   86]\n",
      " [   0   81  146]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      1271\n",
      "           1       0.79      0.78      0.79       543\n",
      "           2       0.58      0.64      0.61       227\n",
      "\n",
      "    accuracy                           0.88      2041\n",
      "   macro avg       0.78      0.80      0.79      2041\n",
      "weighted avg       0.88      0.88      0.88      2041\n",
      "\n",
      "\n",
      "[Japanese] Count Permutation Importance (Top 15) | base_acc=0.8775\n",
      "                        feature  importance_drop_acc\n",
      "       M5Stack1_co2_rate_smooth             0.045566\n",
      "C0A80368-013001_co2_rate_smooth             0.023028\n",
      "                   M5Stack1_voc             0.021231\n",
      "                   M5Stack1_co2             0.020252\n",
      "            C0A80368-013001_co2             0.019925\n",
      "                 M5Stack1_pm2_5             0.015352\n",
      "                M5Stack10_pm2_5             0.013229\n",
      "            C0A80341-013501_gas             0.010126\n",
      "       C0A80368-013001_co2_rate             0.009309\n",
      "                  M5Stack10_co2             0.009146\n",
      "    C0A80341-013501_illuminance             0.004573\n",
      "                   PIR21_motion             0.004573\n",
      "                   M5Stack1_hum             0.003756\n",
      "                  M5Stack1_temp             0.002450\n",
      "              M5Stack1_co2_rate             0.001470\n",
      "\n",
      "[DONE] finished.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SmartHome: Living / Japanese\n",
    "# 入力：対象部屋に設置された家電・センサーだけ（エアコン/空気清浄機/M5Stack/PIR）\n",
    "# タスク：在不在（0/1）と人数（0/1/2）を「別モデル」で学習\n",
    "# 分割：時系列 split（shuffleしない）でリーク抑制\n",
    "# 出力：各epochの train/val loss と lr\n",
    "# 重要度：Permutation Importance（test上で accuracy低下量）\n",
    "#\n",
    "# ✅ NaN/inf対策込み（loss=nan を止める）\n",
    "# ✅ 分散0（一定値）列の自動除去\n",
    "# ✅ 列名揺れ対策：startswith & contains 両方で拾う\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) 設定\n",
    "# ------------------------------------------------------------\n",
    "CSV_PATH = \"./smart_home_features_added.csv\"\n",
    "\n",
    "# 部屋ごとの “使ってよい” デバイス/センサー識別トークン\n",
    "# ※列名に以下の文字列が「含まれる」 or 「先頭一致」する列だけを使う\n",
    "ROOMS = {\n",
    "    \"Living\": {\n",
    "        \"count_label\": \"Label_Living_Count\",\n",
    "        \"tokens\": [\n",
    "            # Living aircon\n",
    "            \"C0A80367-013001_\",\n",
    "            # Living air purifier\n",
    "            \"C0A8033B-013501\",\n",
    "            # M5Stack\n",
    "            \"M5Stack2\",\n",
    "            \"M5Stack8\",\n",
    "            # PIR\n",
    "            \"PIR1\",\n",
    "            \"PIR2\",\n",
    "            \"PIR3\",\n",
    "            \"PIR4\",\n",
    "            \"PIR18\",\n",
    "        ],\n",
    "    },\n",
    "    \"Japanese\": {\n",
    "        \"count_label\": \"Label_Japanese_Count\",\n",
    "        \"tokens\": [\n",
    "            # Japanese aircon\n",
    "            \"C0A80368-013001_\",\n",
    "            # Japanese air purifier\n",
    "            \"C0A80341-013501\",\n",
    "            # M5Stack\n",
    "            \"M5Stack1\",\n",
    "            # PIR\n",
    "            \"PIR17\",\n",
    "            \"PIR21\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 時系列 split 比率（例：70% train / 15% val / 15% test）\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "# 学習設定\n",
    "SEED = 42\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Permutation importance 設定\n",
    "PERM_REPEAT = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) データ読み込み + 安全な欠損処理（超重要）\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "# timestampがあるなら時系列ソート\n",
    "if \"timestamp\" in df.columns:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# ✅ NaN/inf対策（loss=nanの主原因対策）\n",
    "# 1) infをNaNに\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "# 2) 前後で埋める\n",
    "df = df.bfill().ffill()\n",
    "# 3) それでも残るNaNは0（最後の保険）\n",
    "df = df.fillna(0)\n",
    "# 4) FutureWarning対策（型推定）\n",
    "df = df.infer_objects(copy=False)\n",
    "\n",
    "# Label/Action列は特徴量に混ぜない（リーク＆文字列混入防止）\n",
    "label_cols = [c for c in df.columns if c.startswith(\"Label_\")]\n",
    "action_cols = [c for c in df.columns if \"Action\" in c]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) 部屋ごとの特徴量列抽出（startswith & contains）\n",
    "# ------------------------------------------------------------\n",
    "def build_feature_cols(df_room: pd.DataFrame, tokens: list[str]) -> list[str]:\n",
    "    # 数値/Boolだけに限定（文字列列を除去）\n",
    "    numeric_cols = set(df_room.select_dtypes(include=[\"number\", \"bool\"]).columns)\n",
    "\n",
    "    cols = []\n",
    "    for c in df_room.columns:\n",
    "        if c in label_cols or c in action_cols:\n",
    "            continue\n",
    "        if c not in numeric_cols:\n",
    "            continue\n",
    "\n",
    "        # token一致（startswith または contains）\n",
    "        if any(c.startswith(t) for t in tokens) or any(t in c for t in tokens):\n",
    "            cols.append(c)\n",
    "\n",
    "    # bool -> int\n",
    "    for c in cols:\n",
    "        if df_room[c].dtype == bool:\n",
    "            df_room[c] = df_room[c].astype(int)\n",
    "\n",
    "    # 重複除去（順序維持）\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            uniq.append(c)\n",
    "            seen.add(c)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Dataset / Model（単一タスク分類：在不在 or 人数）\n",
    "# ------------------------------------------------------------\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, hidden=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, n_class),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) 学習 / 予測\n",
    "# ------------------------------------------------------------\n",
    "def fit_mlp_classifier(X_train, y_train, X_val, y_val, n_class: int, title: str):\n",
    "    model = MLPClassifier(in_dim=X_train.shape[1], n_class=n_class).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TabularDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TabularDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    print(f\"\\n[{title}] optimizer=AdamW lr={LR} weight_decay={WEIGHT_DECAY}\")\n",
    "    print(f\"[{title}] loss=CrossEntropyLoss\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            # もしここでNaNが出たらデータ側がまだ壊れている\n",
    "            if not torch.isfinite(loss):\n",
    "                print(\n",
    "                    f\"[{title}] ERROR: loss is not finite (NaN/inf). Check X for NaN/inf.\"\n",
    "                )\n",
    "                return model\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        # ---- val ----\n",
    "        model.eval()\n",
    "        va_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                va_losses.append(loss.item())\n",
    "\n",
    "        tr = float(np.mean(tr_losses)) if len(tr_losses) else np.nan\n",
    "        va = float(np.mean(va_losses)) if len(va_losses) else np.nan\n",
    "        print(\n",
    "            f\"[{title}][Epoch {epoch:02d}] train_loss={tr:.4f} val_loss={va:.4f} (lr={optimizer.param_groups[0]['lr']:.2e})\"\n",
    "        )\n",
    "\n",
    "        if va < best_val:\n",
    "            best_val = va\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, X):\n",
    "    model.eval()\n",
    "    # ダミーy（未使用）\n",
    "    loader = DataLoader(\n",
    "        TabularDataset(X, np.zeros(len(X), dtype=int)),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Permutation Importance（accuracy低下量）\n",
    "# ------------------------------------------------------------\n",
    "def permutation_importance_accuracy(\n",
    "    model, X_test, y_test, feature_cols, n_repeat=3, seed=0\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    base_pred = predict(model, X_test)\n",
    "    base_acc = accuracy_score(y_test, base_pred)\n",
    "\n",
    "    importances = []\n",
    "    X_work = X_test.copy()\n",
    "\n",
    "    for j in range(X_test.shape[1]):\n",
    "        drops = []\n",
    "        for _ in range(n_repeat):\n",
    "            saved = X_work[:, j].copy()\n",
    "            rng.shuffle(X_work[:, j])\n",
    "            pred = predict(model, X_work)\n",
    "            acc = accuracy_score(y_test, pred)\n",
    "            drops.append(base_acc - acc)\n",
    "            X_work[:, j] = saved\n",
    "        importances.append(float(np.mean(drops)))\n",
    "\n",
    "    imp = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_cols,\n",
    "            \"importance_drop_acc\": importances,\n",
    "        }\n",
    "    ).sort_values(\"importance_drop_acc\", ascending=False)\n",
    "\n",
    "    return base_acc, imp\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) デバッグ：NaN/inf/分散0チェック（原因特定に効く）\n",
    "# ------------------------------------------------------------\n",
    "def debug_feature_quality(df_room, feature_cols, room_name, task_name):\n",
    "    Xd = df_room[feature_cols].copy()\n",
    "\n",
    "    nan_counts = Xd.isna().sum().sort_values(ascending=False)\n",
    "    inf_counts = np.isinf(Xd.to_numpy()).sum(axis=0)\n",
    "    inf_series = pd.Series(inf_counts, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\n[DEBUG][{room_name}][{task_name}] NaN top5:\")\n",
    "    print(nan_counts.head(5).to_string())\n",
    "    print(f\"[DEBUG][{room_name}][{task_name}] inf top5:\")\n",
    "    print(inf_series.head(5).to_string())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) メイン：部屋ごとに\n",
    "#    - 在不在（2クラス）\n",
    "#    - 人数（3クラス）\n",
    "#    を別々に学習＆重要度算出\n",
    "# ------------------------------------------------------------\n",
    "for room_name, cfg in ROOMS.items():\n",
    "    label_col = cfg[\"count_label\"]\n",
    "    tokens = cfg[\"tokens\"]\n",
    "\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[SKIP] {room_name}: label not found: {label_col}\")\n",
    "        continue\n",
    "\n",
    "    # 実験条件：0/1/2 のみ使う（それ以外があれば除外）\n",
    "    valid = df[label_col].isin([0, 1, 2])\n",
    "    df_room = df.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    # 特徴量列抽出\n",
    "    feature_cols = build_feature_cols(df_room, tokens)\n",
    "    if len(feature_cols) == 0:\n",
    "        print(\n",
    "            f\"[ERROR] {room_name}: feature_cols empty. tokensが列名と合ってない可能性。\"\n",
    "        )\n",
    "        print(\"  tokens =\", tokens)\n",
    "        continue\n",
    "\n",
    "    # 分散0（一定値）列を落とす（学習に不要＆不安定要因）\n",
    "    std = df_room[feature_cols].std(numeric_only=True)\n",
    "    const_cols = std[std == 0].index.tolist()\n",
    "    if const_cols:\n",
    "        print(\n",
    "            f\"[INFO] {room_name}: drop constant cols ({len(const_cols)}): {const_cols}\"\n",
    "        )\n",
    "        feature_cols = [c for c in feature_cols if c not in const_cols]\n",
    "\n",
    "    if len(feature_cols) == 0:\n",
    "        print(f\"[ERROR] {room_name}: all features became constant and were dropped.\")\n",
    "        continue\n",
    "\n",
    "    # ラベル作成\n",
    "    y_count = df_room[label_col].astype(int).values  # 0/1/2\n",
    "    y_occ = (y_count > 0).astype(int)  # 0/1（在不在は別タスク）\n",
    "\n",
    "    # 時系列 split（shuffleなし）\n",
    "    N = len(df_room)\n",
    "    tr_end = int(N * TRAIN_RATIO)\n",
    "    va_end = int(N * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "    X_all = df_room[feature_cols].values\n",
    "\n",
    "    X_tr, X_va, X_te = X_all[:tr_end], X_all[tr_end:va_end], X_all[va_end:]\n",
    "    yc_tr, yc_va, yc_te = y_count[:tr_end], y_count[tr_end:va_end], y_count[va_end:]\n",
    "    yo_tr, yo_va, yo_te = y_occ[:tr_end], y_occ[tr_end:va_end], y_occ[va_end:]\n",
    "\n",
    "    # ✅ scalerは trainでfitのみ（リーク防止）\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # ここで warning/NaN が起きるときは X_tr に NaN/inf が残ってることが多い\n",
    "    # 追加の安全策：X_tr/X_va/X_te の inf を NaN にして0埋め（念のため）\n",
    "    def sanitize_X(X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        X[~np.isfinite(X)] = 0.0\n",
    "        return X\n",
    "\n",
    "    X_tr = sanitize_X(X_tr)\n",
    "    X_va = sanitize_X(X_va)\n",
    "    X_te = sanitize_X(X_te)\n",
    "\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "\n",
    "    # さらに標準化後の NaN/inf も殺す（最後の保険）\n",
    "    X_tr = sanitize_X(X_tr)\n",
    "    X_va = sanitize_X(X_va)\n",
    "    X_te = sanitize_X(X_te)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[ROOM] {room_name}\")\n",
    "    print(\n",
    "        f\"[INFO] label={label_col} | N={N} | train={len(X_tr)} val={len(X_va)} test={len(X_te)}\"\n",
    "    )\n",
    "    print(f\"[INFO] features={len(feature_cols)} (show first 15):\")\n",
    "    print(\n",
    "        \"  \" + \", \".join(feature_cols[:15]) + (\" ...\" if len(feature_cols) > 15 else \"\")\n",
    "    )\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # (A) 在不在モデル（2クラス）\n",
    "    # --------------------------------------------------------\n",
    "    debug_feature_quality(\n",
    "        df_room.iloc[:tr_end], feature_cols, room_name, \"Occupancy(train-slice)\"\n",
    "    )\n",
    "\n",
    "    occ_model = fit_mlp_classifier(\n",
    "        X_tr, yo_tr, X_va, yo_va, n_class=2, title=f\"{room_name}-Occupancy\"\n",
    "    )\n",
    "    occ_pred = predict(occ_model, X_te)\n",
    "\n",
    "    print(\n",
    "        f\"\\n[{room_name}][TEST] Occupancy (0/1) acc = {accuracy_score(yo_te, occ_pred):.4f}\"\n",
    "    )\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(yo_te, occ_pred))\n",
    "    print(\"Report:\\n\", classification_report(yo_te, occ_pred, zero_division=0))\n",
    "\n",
    "    base_acc_occ, imp_occ = permutation_importance_accuracy(\n",
    "        occ_model, X_te, yo_te, feature_cols, n_repeat=PERM_REPEAT, seed=SEED\n",
    "    )\n",
    "    print(\n",
    "        f\"\\n[{room_name}] Occupancy Permutation Importance (Top 15) | base_acc={base_acc_occ:.4f}\"\n",
    "    )\n",
    "    print(imp_occ.head(15).to_string(index=False))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # (B) 人数モデル（3クラス：0/1/2）\n",
    "    # --------------------------------------------------------\n",
    "    debug_feature_quality(\n",
    "        df_room.iloc[:tr_end], feature_cols, room_name, \"Count(train-slice)\"\n",
    "    )\n",
    "\n",
    "    cnt_model = fit_mlp_classifier(\n",
    "        X_tr, yc_tr, X_va, yc_va, n_class=3, title=f\"{room_name}-Count(0/1/2)\"\n",
    "    )\n",
    "    cnt_pred = predict(cnt_model, X_te)\n",
    "\n",
    "    print(\n",
    "        f\"\\n[{room_name}][TEST] Count (0/1/2) acc = {accuracy_score(yc_te, cnt_pred):.4f}\"\n",
    "    )\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(yc_te, cnt_pred))\n",
    "    print(\"Report:\\n\", classification_report(yc_te, cnt_pred, zero_division=0))\n",
    "\n",
    "    base_acc_cnt, imp_cnt = permutation_importance_accuracy(\n",
    "        cnt_model, X_te, yc_te, feature_cols, n_repeat=PERM_REPEAT, seed=SEED\n",
    "    )\n",
    "    print(\n",
    "        f\"\\n[{room_name}] Count Permutation Importance (Top 15) | base_acc={base_acc_cnt:.4f}\"\n",
    "    )\n",
    "    print(imp_cnt.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n[DONE] finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2957989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/8k_9t5_n3sng20r8pshmdzmr0000gn/T/ipykernel_20889/1608246423.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.bfill().ffill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Living: drop constant cols (7): ['C0A8033B-013501_opStatus', 'C0A8033B-013501_flow', 'C0A80367-013001_mode', 'C0A80367-013001_blowTemp', 'C0A80367-013001_flow', 'C0A80367-013001_blowTemp_rate', 'C0A80367-013001_blowTemp_rate_smooth']\n",
      "\n",
      "==========================================================================================\n",
      "[ROOM] Living\n",
      "[INFO] label=Label_Living_Count | N=13606 | features=72\n",
      "[INFO] features first 15: ['PIR1_motion', 'PIR2_motion', 'PIR3_motion', 'PIR4_motion', 'PIR18_motion', 'PIR13_motion', 'PIR11_motion', 'PIR21_motion', 'PIR17_motion', 'PIR10_motion', 'PIR15_motion', 'PIR19_motion', 'PIR20_motion', 'PIR22_motion', 'PIR24_motion']\n",
      "==========================================================================================\n",
      "\n",
      "[Living][Occupancy(0/1)][TEST] acc = 0.6193\n",
      "Confusion matrix:\n",
      " [[1085    0]\n",
      " [ 777  179]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74      1085\n",
      "           1       1.00      0.19      0.32       956\n",
      "\n",
      "    accuracy                           0.62      2041\n",
      "   macro avg       0.79      0.59      0.53      2041\n",
      "weighted avg       0.78      0.62      0.54      2041\n",
      "\n",
      "\n",
      "[Living][Occupancy(0/1)] RF Gini importance (Top 15)\n",
      "                    feature  gini_importance\n",
      "C0A8033B-013501_illuminance         0.134379\n",
      "      C0A80367-013001_human         0.078572\n",
      "               M5Stack8_hum         0.074436\n",
      "        C0A80367-013001_co2         0.063819\n",
      "               M5Stack2_co2         0.056590\n",
      "              M5Stack8_temp         0.049278\n",
      "               M5Stack2_voc         0.049126\n",
      "   M5Stack8_co2_rate_smooth         0.046098\n",
      "               M5Stack2_hum         0.044300\n",
      "               M5Stack8_co2         0.043849\n",
      "   M5Stack2_co2_rate_smooth         0.038568\n",
      "        C0A8033B-013501_hum         0.035988\n",
      "   M5Stack8_voc_rate_smooth         0.034063\n",
      " C0A80367-013001_totalPower         0.030165\n",
      "               M5Stack8_voc         0.028328\n",
      "\n",
      "[Living][Occupancy(0/1)] Permutation importance on TEST (Top 15)\n",
      "                        feature  perm_importance_mean_drop_acc  perm_importance_std\n",
      "                   M5Stack2_hum                       0.032827             0.000980\n",
      "            C0A8033B-013501_hum                       0.028515             0.002018\n",
      "    C0A8033B-013501_illuminance                       0.027829             0.001329\n",
      "                  M5Stack8_temp                       0.013327             0.001046\n",
      "           C0A8033B-013501_temp                       0.012445             0.000796\n",
      "                  M5Stack2_temp                       0.010289             0.000693\n",
      "       M5Stack2_voc_rate_smooth                       0.007055             0.000500\n",
      "       M5Stack2_co2_rate_smooth                       0.006663             0.000392\n",
      "          C0A80367-013001_human                       0.006467             0.001531\n",
      "       M5Stack8_voc_rate_smooth                       0.005879             0.000820\n",
      "C0A80367-013001_co2_rate_smooth                       0.004018             0.000950\n",
      "                    PIR4_motion                       0.003430             0.001278\n",
      "                   M5Stack8_hum                       0.003430             0.001697\n",
      "       M5Stack8_co2_rate_smooth                       0.003038             0.000784\n",
      "            C0A8033B-013501_gas                       0.002548             0.000367\n",
      "\n",
      "[Living][Count(0/1/2)][TEST] acc = 0.5728\n",
      "Confusion matrix:\n",
      " [[1085    0    0]\n",
      " [ 709   84    1]\n",
      " [ 162    0    0]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71      1085\n",
      "           1       1.00      0.11      0.19       794\n",
      "           2       0.00      0.00      0.00       162\n",
      "\n",
      "    accuracy                           0.57      2041\n",
      "   macro avg       0.52      0.37      0.30      2041\n",
      "weighted avg       0.68      0.57      0.45      2041\n",
      "\n",
      "\n",
      "[Living][Count(0/1/2)] RF Gini importance (Top 15)\n",
      "                    feature  gini_importance\n",
      "C0A8033B-013501_illuminance         0.116421\n",
      "        C0A80367-013001_co2         0.076596\n",
      "               M5Stack2_co2         0.072747\n",
      "               M5Stack2_voc         0.057352\n",
      "               M5Stack8_voc         0.053921\n",
      "               M5Stack8_co2         0.052781\n",
      "        C0A8033B-013501_hum         0.051116\n",
      " C0A80367-013001_totalPower         0.050792\n",
      "      C0A80367-013001_human         0.049736\n",
      "               M5Stack8_hum         0.048514\n",
      "              M5Stack8_temp         0.045091\n",
      "               M5Stack2_hum         0.035813\n",
      "   C0A80367-013001_sunshine         0.025524\n",
      "C0A80367-013001_outsideTemp         0.024028\n",
      "       C0A8033B-013501_temp         0.022975\n",
      "\n",
      "[Living][Count(0/1/2)] Permutation importance on TEST (Top 15)\n",
      "                    feature  perm_importance_mean_drop_acc  perm_importance_std\n",
      "               M5Stack2_hum                       0.023812             0.001686\n",
      "        C0A8033B-013501_hum                       0.019696             0.001365\n",
      "C0A8033B-013501_illuminance                       0.015875             0.001537\n",
      "               M5Stack2_voc                       0.009995             0.002115\n",
      "       C0A8033B-013501_temp                       0.005390             0.000310\n",
      "        C0A80367-013001_co2                       0.005390             0.001073\n",
      "        C0A80367-013001_hum                       0.004410             0.001697\n",
      "              M5Stack2_temp                       0.003920             0.000876\n",
      "   M5Stack8_voc_rate_smooth                       0.002646             0.000665\n",
      "              M5Stack8_temp                       0.002156             0.000392\n",
      "   M5Stack8_hum_rate_smooth                       0.002058             0.000196\n",
      "               M5Stack2_co2                       0.001862             0.000898\n",
      "   M5Stack2_voc_rate_smooth                       0.001666             0.000500\n",
      "   M5Stack8_co2_rate_smooth                       0.000490             0.000310\n",
      "   M5Stack2_co2_rate_smooth                       0.000392             0.000571\n",
      "[INFO] Japanese: drop constant cols (7): ['C0A80341-013501_opStatus', 'C0A80341-013501_flow', 'C0A80368-013001_mode', 'C0A80368-013001_blowTemp', 'C0A80368-013001_flow', 'C0A80368-013001_blowTemp_rate', 'C0A80368-013001_blowTemp_rate_smooth']\n",
      "\n",
      "==========================================================================================\n",
      "[ROOM] Japanese\n",
      "[INFO] label=Label_Japanese_Count | N=13606 | features=59\n",
      "[INFO] features first 15: ['PIR21_motion', 'PIR17_motion', 'M5Stack1_co2', 'M5Stack1_temp', 'M5Stack1_hum', 'M5Stack1_pm2_5', 'M5Stack1_voc', 'M5Stack10_co2', 'M5Stack10_temp', 'M5Stack10_hum', 'M5Stack10_pm2_5', 'M5Stack10_voc', 'C0A80341-013501_temp', 'C0A80341-013501_hum', 'C0A80341-013501_pm25']\n",
      "==========================================================================================\n",
      "\n",
      "[Japanese][Occupancy(0/1)][TEST] acc = 0.9245\n",
      "Confusion matrix:\n",
      " [[1248   23]\n",
      " [ 131  639]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      1271\n",
      "           1       0.97      0.83      0.89       770\n",
      "\n",
      "    accuracy                           0.92      2041\n",
      "   macro avg       0.94      0.91      0.92      2041\n",
      "weighted avg       0.93      0.92      0.92      2041\n",
      "\n",
      "\n",
      "[Japanese][Occupancy(0/1)] RF Gini importance (Top 15)\n",
      "                        feature  gini_importance\n",
      "          C0A80368-013001_human         0.118271\n",
      "                   M5Stack1_co2         0.108413\n",
      "            C0A80368-013001_co2         0.104693\n",
      "       M5Stack1_co2_rate_smooth         0.078753\n",
      "                   M5Stack1_voc         0.074626\n",
      "C0A80368-013001_co2_rate_smooth         0.059574\n",
      "     C0A80368-013001_totalPower         0.049078\n",
      "    C0A80341-013501_illuminance         0.043879\n",
      "                  M5Stack10_voc         0.042308\n",
      "                 M5Stack10_temp         0.035135\n",
      "       M5Stack1_voc_rate_smooth         0.029481\n",
      "                  M5Stack10_co2         0.029266\n",
      "            C0A80341-013501_gas         0.018523\n",
      "                   M5Stack1_hum         0.018507\n",
      "       C0A80368-013001_co2_rate         0.016005\n",
      "\n",
      "[Japanese][Occupancy(0/1)] Permutation importance on TEST (Top 15)\n",
      "                        feature  perm_importance_mean_drop_acc  perm_importance_std\n",
      "    C0A80341-013501_illuminance                       0.041940             0.003752\n",
      "                   M5Stack1_co2                       0.030867             0.001610\n",
      "       M5Stack1_voc_rate_smooth                       0.029593             0.002622\n",
      "       M5Stack1_co2_rate_smooth                       0.027144             0.002917\n",
      "            C0A80368-013001_co2                       0.019206             0.003074\n",
      "          C0A80368-013001_human                       0.016463             0.002023\n",
      "                  M5Stack10_co2                       0.012445             0.001184\n",
      "          C0A80341-013501_power                       0.010387             0.001329\n",
      "C0A80368-013001_co2_rate_smooth                       0.009407             0.004463\n",
      "                  M5Stack10_voc                       0.003822             0.001255\n",
      "            C0A80341-013501_gas                       0.003626             0.001999\n",
      "                   M5Stack1_hum                       0.003528             0.001216\n",
      "              M5Stack1_voc_rate                       0.002842             0.000843\n",
      "            C0A80341-013501_hum                       0.001372             0.000367\n",
      "           C0A80341-013501_odor                       0.001274             0.000733\n",
      "\n",
      "[Japanese][Count(0/1/2)][TEST] acc = 0.7952\n",
      "Confusion matrix:\n",
      " [[1265    0    6]\n",
      " [ 207  225  111]\n",
      " [   2   92  133]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      1271\n",
      "           1       0.71      0.41      0.52       543\n",
      "           2       0.53      0.59      0.56       227\n",
      "\n",
      "    accuracy                           0.80      2041\n",
      "   macro avg       0.70      0.67      0.67      2041\n",
      "weighted avg       0.78      0.80      0.78      2041\n",
      "\n",
      "\n",
      "[Japanese][Count(0/1/2)] RF Gini importance (Top 15)\n",
      "                        feature  gini_importance\n",
      "            C0A80368-013001_co2         0.076722\n",
      "                  M5Stack10_co2         0.071536\n",
      "                   M5Stack1_co2         0.071004\n",
      "     C0A80368-013001_totalPower         0.069646\n",
      "          C0A80368-013001_human         0.069284\n",
      "C0A80368-013001_co2_rate_smooth         0.061674\n",
      "                  M5Stack10_voc         0.061374\n",
      "                   M5Stack1_voc         0.055899\n",
      "                   M5Stack1_hum         0.046304\n",
      "       M5Stack1_co2_rate_smooth         0.045108\n",
      "                 M5Stack10_temp         0.040145\n",
      "                  M5Stack1_temp         0.035289\n",
      "    C0A80341-013501_illuminance         0.034474\n",
      "            C0A80341-013501_hum         0.027223\n",
      "    C0A80368-013001_outsideTemp         0.023088\n",
      "\n",
      "[Japanese][Count(0/1/2)] Permutation importance on TEST (Top 15)\n",
      "                        feature  perm_importance_mean_drop_acc  perm_importance_std\n",
      "            C0A80368-013001_co2                       0.052229             0.002046\n",
      "                   M5Stack1_co2                       0.050563             0.001499\n",
      "                  M5Stack10_co2                       0.047624             0.001499\n",
      "          C0A80368-013001_human                       0.047428             0.001499\n",
      "    C0A80341-013501_illuminance                       0.035375             0.003329\n",
      "       M5Stack1_co2_rate_smooth                       0.033219             0.003788\n",
      "       M5Stack1_voc_rate_smooth                       0.030671             0.001950\n",
      "                  M5Stack1_temp                       0.027242             0.002138\n",
      "C0A80368-013001_co2_rate_smooth                       0.025184             0.003199\n",
      "              M5Stack1_voc_rate                       0.009799             0.000693\n",
      "                  M5Stack10_voc                       0.005096             0.001714\n",
      "                   M5Stack1_hum                       0.003920             0.001885\n",
      "                   M5Stack1_voc                       0.003724             0.002269\n",
      "             M5Stack1_temp_rate                       0.002156             0.000796\n",
      "            C0A80368-013001_hum                       0.002156             0.001143\n",
      "\n",
      "[DONE] RandomForest finished.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RandomForest版\n",
    "# - Living / Japanese を部屋ごとに学習\n",
    "# - 入力：対象部屋に設置された家電・センサーだけ（エアコン/空気清浄機/M5Stack/PIR）\n",
    "# - タスク：在不在（0/1）と人数（0/1/2）を別モデル\n",
    "# - 分割：時系列 split（shuffleしない）でリーク抑制\n",
    "# - test(正解データ)は学習/前処理で見ない（fitはtrainだけ）\n",
    "# - 特徴量重要度：Permutation Importance（推奨） + RF組み込み(Gini)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) 設定\n",
    "# ------------------------------------------------------------\n",
    "CSV_PATH = \"./smart_home_features_added.csv\"\n",
    "\n",
    "ROOMS = {\n",
    "    \"Living\": {\n",
    "        \"count_label\": \"Label_Living_Count\",\n",
    "        \"tokens\": [\n",
    "            \"C0A80367-013001_\",  # Living aircon\n",
    "            \"C0A8033B-013501\",  # Living air purifier\n",
    "            \"M5Stack2\",\n",
    "            \"M5Stack8\",\n",
    "            \"PIR1\",\n",
    "            \"PIR2\",\n",
    "            \"PIR3\",\n",
    "            \"PIR4\",\n",
    "            \"PIR18\",\n",
    "        ],\n",
    "    },\n",
    "    \"Japanese\": {\n",
    "        \"count_label\": \"Label_Japanese_Count\",\n",
    "        \"tokens\": [\n",
    "            \"C0A80368-013001_\",  # Japanese aircon\n",
    "            \"C0A80341-013501\",  # Japanese air purifier\n",
    "            \"M5Stack1\",\n",
    "            \"PIR17\",\n",
    "            \"PIR21\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15  # RFではvalを必須にしないが、同じsplitを維持（testを厳密に隔離）\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# RF設定（必要なら調整）\n",
    "RF_COMMON = dict(\n",
    "    n_estimators=600,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\",  # クラス不均衡に少し強く\n",
    ")\n",
    "\n",
    "# Permutation importance\n",
    "PERM_REPEAT = 5\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) データ読み込み + NaN/inf対策\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "if \"timestamp\" in df.columns:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.bfill().ffill()\n",
    "df = df.fillna(0)\n",
    "df = df.infer_objects(copy=False)\n",
    "\n",
    "label_cols = [c for c in df.columns if c.startswith(\"Label_\")]\n",
    "action_cols = [c for c in df.columns if \"Action\" in c]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) 特徴量列抽出（startswith & contains 両対応）\n",
    "# ------------------------------------------------------------\n",
    "def build_feature_cols(df_room: pd.DataFrame, tokens: list[str]) -> list[str]:\n",
    "    numeric_cols = set(df_room.select_dtypes(include=[\"number\", \"bool\"]).columns)\n",
    "\n",
    "    cols = []\n",
    "    for c in df_room.columns:\n",
    "        if c in label_cols or c in action_cols:\n",
    "            continue\n",
    "        if c not in numeric_cols:\n",
    "            continue\n",
    "        if any(c.startswith(t) for t in tokens) or any(t in c for t in tokens):\n",
    "            cols.append(c)\n",
    "\n",
    "    # bool -> int\n",
    "    for c in cols:\n",
    "        if df_room[c].dtype == bool:\n",
    "            df_room[c] = df_room[c].astype(int)\n",
    "\n",
    "    # 重複除去\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            uniq.append(c)\n",
    "            seen.add(c)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "def sanitize_X(X):\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    X[~np.isfinite(X)] = 0.0\n",
    "    return X\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) 学習・評価（testを絶対に学習に使わない）\n",
    "# ------------------------------------------------------------\n",
    "def train_eval_rf(\n",
    "    room_name: str,\n",
    "    df_room: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    y,\n",
    "    task_name: str,\n",
    "    n_class: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    df_room: 既に時系列順 & 対象ラベルだけ残したDF\n",
    "    y: 目的変数（np.array）\n",
    "    \"\"\"\n",
    "    N = len(df_room)\n",
    "    tr_end = int(N * TRAIN_RATIO)\n",
    "    va_end = int(N * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "    X_all = sanitize_X(df_room[feature_cols].values)\n",
    "\n",
    "    # ✅ 時系列split（shuffleしない）\n",
    "    X_tr, X_va, X_te = X_all[:tr_end], X_all[tr_end:va_end], X_all[va_end:]\n",
    "    y_tr, y_va, y_te = y[:tr_end], y[tr_end:va_end], y[va_end:]\n",
    "\n",
    "    # ✅ ここが重要：fitはtrainだけ（testは一切使わない）\n",
    "    model = RandomForestClassifier(**RF_COMMON)\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # 評価（valは参考、testが本番）\n",
    "    pred_te = model.predict(X_te)\n",
    "\n",
    "    print(\n",
    "        f\"\\n[{room_name}][{task_name}][TEST] acc = {accuracy_score(y_te, pred_te):.4f}\"\n",
    "    )\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_te, pred_te))\n",
    "    print(\"Report:\\n\", classification_report(y_te, pred_te, zero_division=0))\n",
    "\n",
    "    # --- 重要度1：RF組み込み（Gini重要度） ---\n",
    "    gini = pd.DataFrame(\n",
    "        {\"feature\": feature_cols, \"gini_importance\": model.feature_importances_}\n",
    "    ).sort_values(\"gini_importance\", ascending=False)\n",
    "\n",
    "    print(f\"\\n[{room_name}][{task_name}] RF Gini importance (Top 15)\")\n",
    "    print(gini.head(15).to_string(index=False))\n",
    "\n",
    "    # --- 重要度2：Permutation Importance（推奨 / testで計算） ---\n",
    "    # ※testを“学習に使わない”のは守ったまま、評価として重要度を算出\n",
    "    perm = permutation_importance(\n",
    "        model,\n",
    "        X_te,\n",
    "        y_te,\n",
    "        n_repeats=PERM_REPEAT,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    perm_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_cols,\n",
    "            \"perm_importance_mean_drop_acc\": perm.importances_mean,\n",
    "            \"perm_importance_std\": perm.importances_std,\n",
    "        }\n",
    "    ).sort_values(\"perm_importance_mean_drop_acc\", ascending=False)\n",
    "\n",
    "    print(f\"\\n[{room_name}][{task_name}] Permutation importance on TEST (Top 15)\")\n",
    "    print(perm_df.head(15).to_string(index=False))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) メイン：部屋ごとに在不在RF & 人数RF\n",
    "# ------------------------------------------------------------\n",
    "for room_name, cfg in ROOMS.items():\n",
    "    label_col = cfg[\"count_label\"]\n",
    "    tokens = cfg[\"tokens\"]\n",
    "\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[SKIP] {room_name}: label not found: {label_col}\")\n",
    "        continue\n",
    "\n",
    "    # 実験条件：0/1/2 だけに限定\n",
    "    valid = df[label_col].isin([0, 1, 2])\n",
    "    df_room = df.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    feature_cols = build_feature_cols(df_room, tokens)\n",
    "    if len(feature_cols) == 0:\n",
    "        print(\n",
    "            f\"[ERROR] {room_name}: feature_cols empty. tokensが列名と合ってない可能性。\"\n",
    "        )\n",
    "        print(\"  tokens =\", tokens)\n",
    "        continue\n",
    "\n",
    "    # 分散0（一定値）列を落とす（RFでも不要）\n",
    "    std = df_room[feature_cols].std(numeric_only=True)\n",
    "    const_cols = std[std == 0].index.tolist()\n",
    "    if const_cols:\n",
    "        print(\n",
    "            f\"[INFO] {room_name}: drop constant cols ({len(const_cols)}): {const_cols}\"\n",
    "        )\n",
    "        feature_cols = [c for c in feature_cols if c not in const_cols]\n",
    "\n",
    "    if len(feature_cols) == 0:\n",
    "        print(f\"[ERROR] {room_name}: all features became constant and were dropped.\")\n",
    "        continue\n",
    "\n",
    "    # ラベル作成\n",
    "    y_count = df_room[label_col].astype(int).values  # 0/1/2\n",
    "    y_occ = (y_count > 0).astype(int)  # 在不在 0/1（別タスク）\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"[ROOM] {room_name}\")\n",
    "    print(f\"[INFO] label={label_col} | N={len(df_room)} | features={len(feature_cols)}\")\n",
    "    print(f\"[INFO] features first 15: {feature_cols[:15]}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    # (A) 在不在（2値）RF\n",
    "    _ = train_eval_rf(\n",
    "        room_name=room_name,\n",
    "        df_room=df_room,\n",
    "        feature_cols=feature_cols,\n",
    "        y=y_occ,\n",
    "        task_name=\"Occupancy(0/1)\",\n",
    "        n_class=2,\n",
    "    )\n",
    "\n",
    "    # (B) 人数（0/1/2）RF\n",
    "    _ = train_eval_rf(\n",
    "        room_name=room_name,\n",
    "        df_room=df_room,\n",
    "        feature_cols=feature_cols,\n",
    "        y=y_count,\n",
    "        task_name=\"Count(0/1/2)\",\n",
    "        n_class=3,\n",
    "    )\n",
    "\n",
    "print(\"\\n[DONE] RandomForest finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e4e92b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     accuracy_score,\n\u001b[32m     33\u001b[39m     balanced_accuracy_score,\n\u001b[32m     34\u001b[39m     f1_score,\n\u001b[32m     35\u001b[39m     confusion_matrix,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclass_weight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 設定\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     45\u001b[39m CSV_PATH = \u001b[33m\"\u001b[39m\u001b[33m./smart_home_renamed_with_timestamp.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# あなたが作ったtimestamp付きCSV\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RandomForest / LightGBM\n",
    "# - 時系列split（timestamp順：前をtrain、後ろをtest）\n",
    "# - データ拡張（trainのみ）：ノイズ + タイムシフト\n",
    "# - 追加特徴量（CO2/PM2.5/VOCなど数値系）\n",
    "#    window = [5,10,15,20,25,30,60] 分で一括作成\n",
    "#    - diff / slope / rolling mean/std / integral(sum)\n",
    "#    - trend（rolling回帰傾き）\n",
    "#    - monotonic_ratio（上昇している割合）\n",
    "#    - acceleration（傾きの変化）\n",
    "#    - area（基準値からの累積面積）\n",
    "# - 各部屋（Living/Japanese）\n",
    "# - 3条件を回す（要望対応）\n",
    "#    A) all：家電 + センサー（M5Stack + PIR）\n",
    "#    B) appliance_only：家電(C0A8033B/C0A80341/C0A80367/C0A80368)のみ（PIRなし）\n",
    "#    C) no_co2_human：CO2とHuman(PIR/human)を除外した特徴（PM2.5/VOC/温湿度等）\n",
    "# - 出力：Accuracy / BalancedAcc / F1macro / Confusion（matplotlib）\n",
    "# - feature importance：各条件「全特徴」をCSV保存 + TopKをPNG保存（カレント直下）\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 設定\n",
    "# =========================\n",
    "CSV_PATH = \"./smart_home_renamed_with_timestamp.csv\"  # あなたが作ったtimestamp付きCSV\n",
    "TIME_COL = \"timestamp\"\n",
    "RESAMPLE_RULE = \"1min\"  # 1分に揃える（特徴量が安定）\n",
    "TRAIN_RATIO = 0.80  # 時系列split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# データ拡張（trainのみ）\n",
    "AUG_NOISE_LEVEL = 0.02\n",
    "AUG_SHIFT_STEPS = [-1, 1]  # 1分ずらし\n",
    "AUG_REPEAT_NOISE = 1  # ノイズを何回作るか\n",
    "\n",
    "# 特徴量window\n",
    "WINS = [5, 10, 15, 20, 25, 30, 60]\n",
    "\n",
    "# importance描画\n",
    "TOPK_IMPORTANCE = 30\n",
    "\n",
    "# 目的変数は 0/1/2 に固定（それ以外は捨てる）\n",
    "ALLOWED_Y = [0, 1, 2]\n",
    "\n",
    "# 家電prefix（重要：あなたの要望の家電だけ）\n",
    "APPLIANCE_PREFIXES = (\"C0A8033B\", \"C0A80341\", \"C0A80367\", \"C0A80368\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ユーティリティ\n",
    "# =========================\n",
    "def to_dt(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(series, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "\n",
    "def safe_ffill(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 未来を見ない：ffillのみ\n",
    "    return df.ffill().infer_objects(copy=False)\n",
    "\n",
    "\n",
    "def resample_1min(df: pd.DataFrame, rule=\"1min\") -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    other_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "    out = []\n",
    "    if num_cols:\n",
    "        out.append(df[num_cols].resample(rule).mean())\n",
    "    if other_cols:\n",
    "        out.append(df[other_cols].resample(rule).last())\n",
    "\n",
    "    dfr = pd.concat(out, axis=1).sort_index()\n",
    "    dfr = safe_ffill(dfr)\n",
    "    return dfr\n",
    "\n",
    "\n",
    "def time_split_idx(n: int, train_ratio: float):\n",
    "    n_tr = int(n * train_ratio)\n",
    "    tr_idx = np.arange(0, n_tr)\n",
    "    te_idx = np.arange(n_tr, n)\n",
    "    return tr_idx, te_idx\n",
    "\n",
    "\n",
    "def coerce_boolish_to_float(s: pd.Series) -> pd.Series:\n",
    "    s = s.replace({\"True\": 1, \"False\": 0, True: 1, False: 0})\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s.astype(float)\n",
    "\n",
    "\n",
    "def compact_metrics(y_true, y_pred, labels):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return acc, bacc, f1m, cm\n",
    "\n",
    "\n",
    "def plot_confusion_mat(cm, labels, title, save_png):\n",
    "    plt.figure(figsize=(5.2, 4.2))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "\n",
    "    # annotate\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {save_png}\")\n",
    "\n",
    "\n",
    "def save_importance_csv_and_plot(feature_names, importances, title, tag, topk=30):\n",
    "    \"\"\"\n",
    "    feature importanceを\n",
    "    - 全特徴CSV保存\n",
    "    - 上位topkをPNG保存\n",
    "    \"\"\"\n",
    "    imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    csv_path = f\"{tag}_importance.csv\"\n",
    "    imp.reset_index().rename(columns={\"index\": \"feature\", 0: \"importance\"}).to_csv(\n",
    "        csv_path, index=False\n",
    "    )\n",
    "    print(f\"[Saved] {csv_path}\")\n",
    "\n",
    "    s = imp.head(topk)[::-1]\n",
    "    plt.figure(figsize=(10, max(4, 0.28 * len(s))))\n",
    "    plt.barh(s.index, s.values)\n",
    "    plt.title(f\"{title} (Top{topk})\")\n",
    "    plt.xlabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    png_path = f\"{tag}_importance.png\"\n",
    "    plt.savefig(png_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {png_path}\")\n",
    "\n",
    "\n",
    "def compute_sample_weight_from_y(y: np.ndarray):\n",
    "    # 明示 class weight（sklearn系）\n",
    "    classes = np.unique(y)\n",
    "    cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "    cw_map = {c: w for c, w in zip(classes, cw)}\n",
    "    return np.array([cw_map[v] for v in y], dtype=float)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 特徴量（数値系）生成\n",
    "# =========================\n",
    "def _rolling_linreg_slope(y: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    rolling linear regression slope over last `window` points.\n",
    "    equally spaced (1min). unit: value/min\n",
    "    \"\"\"\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    w = int(window)\n",
    "    if w < 3:\n",
    "        return pd.Series(0.0, index=y.index)\n",
    "\n",
    "    t = np.arange(w, dtype=float)\n",
    "    t_mean = t.mean()\n",
    "    t_centered = t - t_mean\n",
    "    denom = np.sum(t_centered**2)\n",
    "\n",
    "    def slope_fn(a):\n",
    "        a = np.asarray(a, dtype=float)\n",
    "        a_mean = np.mean(a)\n",
    "        return np.dot(t_centered, (a - a_mean)) / denom\n",
    "\n",
    "    return y.rolling(w, min_periods=w).apply(slope_fn, raw=True).fillna(0.0)\n",
    "\n",
    "\n",
    "def _monotonic_ratio(y: pd.Series, window: int) -> pd.Series:\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    d = y.diff()\n",
    "    pos = (d > 0).astype(float)\n",
    "    return pos.rolling(window, min_periods=1).mean().fillna(0.0)\n",
    "\n",
    "\n",
    "def _area_from_baseline(y: pd.Series, window: int) -> pd.Series:\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    base = y.rolling(window, min_periods=1).min()\n",
    "    area = (y - base).rolling(window, min_periods=1).sum()\n",
    "    return area.fillna(0.0)\n",
    "\n",
    "\n",
    "def build_numeric_features_multiwin(\n",
    "    df: pd.DataFrame, cols: list[str], wins: list[int]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    任意の数値列に対し、winsに対する特徴量を一気に作成。\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\").astype(float).ffill()\n",
    "\n",
    "        # raw\n",
    "        feats[c] = s\n",
    "\n",
    "        # 1min diff\n",
    "        feats[f\"{c}_diff_1min\"] = s.diff().fillna(0.0)\n",
    "\n",
    "        for w in wins:\n",
    "            # rolling stats\n",
    "            feats[f\"{c}_mean_{w}min\"] = s.rolling(w, min_periods=1).mean().fillna(0.0)\n",
    "            feats[f\"{c}_std_{w}min\"] = s.rolling(w, min_periods=1).std().fillna(0.0)\n",
    "\n",
    "            # slope approx (ppm/min等)\n",
    "            feats[f\"{c}_slope_{w}min\"] = ((s - s.shift(w)) / float(w)).fillna(0.0)\n",
    "\n",
    "            # integral (sum over window)\n",
    "            feats[f\"{c}_integral_{w}min\"] = (\n",
    "                s.rolling(w, min_periods=1).sum().fillna(0.0)\n",
    "            )\n",
    "\n",
    "            # trend via rolling regression (requested)\n",
    "            feats[f\"{c}_trend_{w}min\"] = _rolling_linreg_slope(s, w)\n",
    "\n",
    "            # monotonic ratio\n",
    "            feats[f\"{c}_monotonic_ratio_{w}min\"] = _monotonic_ratio(s, w)\n",
    "\n",
    "            # area from baseline\n",
    "            feats[f\"{c}_area_{w}min\"] = _area_from_baseline(s, w)\n",
    "\n",
    "            # acceleration: slope change（短窓-長窓っぽい差分）\n",
    "            # ここでは「1min diffのw分平均との差」を簡易加速度として入れる\n",
    "            diff = feats[f\"{c}_diff_1min\"]\n",
    "            feats[f\"{c}_accel_{w}min\"] = (\n",
    "                diff - diff.rolling(w, min_periods=1).mean()\n",
    "            ).fillna(0.0)\n",
    "\n",
    "    X = pd.DataFrame(feats, index=df.index)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def build_human_features(\n",
    "    df: pd.DataFrame, human_cols: list[str], wins: list[int]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    human / PIR など boolish を数値化して rolling sum を付与\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    for c in human_cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = coerce_boolish_to_float(df[c]).ffill().fillna(0.0)\n",
    "        feats[c] = s\n",
    "        for w in wins:\n",
    "            feats[f\"{c}_sum_{w}min\"] = s.rolling(w, min_periods=1).sum().fillna(0.0)\n",
    "    X = pd.DataFrame(feats, index=df.index)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 列選択（部屋ごと）\n",
    "# =========================\n",
    "def is_pm25(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    return (\"pm2_5\" in n) or (\"pm2.5\" in n) or (\"pm25\" in n)\n",
    "\n",
    "\n",
    "def is_co2(name: str) -> bool:\n",
    "    return name.lower().endswith(\"_co2\") or (\"_co2_\" in name.lower())\n",
    "\n",
    "\n",
    "def is_voc(name: str) -> bool:\n",
    "    return \"voc\" in name.lower()\n",
    "\n",
    "\n",
    "def is_human_like(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    return (\"pir\" in n) or n.endswith(\"_human\") or (\"motion\" in n)\n",
    "\n",
    "\n",
    "def is_temp_hum_like(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    return (\"_temp\" in n) or (\"temp\" in n) or (\"_hum\" in n) or (\"humid\" in n)\n",
    "\n",
    "\n",
    "def uniq(xs):\n",
    "    return list(dict.fromkeys(xs))\n",
    "\n",
    "\n",
    "def pick_room_columns(df: pd.DataFrame, room: str):\n",
    "    \"\"\"\n",
    "    Living:\n",
    "      - label: Label_Living_Count\n",
    "      - house appliances: C0A80367-013001_ (AC), C0A8033B-013501_ (purifier)\n",
    "      - sensors: M5Stack1/2/8 + PIR*\n",
    "    Japanese:\n",
    "      - label: Label_Japanese_Count\n",
    "      - appliances: C0A80368-013001_ (AC), C0A80341-013501_ (purifier)\n",
    "      - sensors: M5Stack1/2/8 + PIR*\n",
    "    \"\"\"\n",
    "    if room == \"Living\":\n",
    "        label = \"Label_Living_Count\"\n",
    "        appliance_cols = [\n",
    "            c\n",
    "            for c in df.columns\n",
    "            if c.startswith(\"C0A80367-013001_\") or c.startswith(\"C0A8033B-013501_\")\n",
    "        ]\n",
    "    elif room == \"Japanese\":\n",
    "        label = \"Label_Japanese_Count\"\n",
    "        appliance_cols = [\n",
    "            c\n",
    "            for c in df.columns\n",
    "            if c.startswith(\"C0A80368-013001_\") or c.startswith(\"C0A80341-013501_\")\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"room must be Living or Japanese\")\n",
    "\n",
    "    # M5Stack subset（要求：1/2/8）\n",
    "    m5_cols = [\n",
    "        c\n",
    "        for c in df.columns\n",
    "        if (\n",
    "            c.startswith(\"M5Stack1_\")\n",
    "            or c.startswith(\"M5Stack2_\")\n",
    "            or c.startswith(\"M5Stack8_\")\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # PIR cols（そのまま拾う）\n",
    "    pir_cols = [c for c in df.columns if c.lower().startswith(\"pir\")]\n",
    "\n",
    "    # 数値センサ系（部屋に関係しそうなもの：家電 + M5Stack）\n",
    "    candidate_num = appliance_cols + m5_cols\n",
    "\n",
    "    co2_cols = [c for c in candidate_num if is_co2(c)]\n",
    "    pm25_cols = [c for c in candidate_num if is_pm25(c)]\n",
    "    voc_cols = [c for c in candidate_num if is_voc(c)]\n",
    "    th_cols = [c for c in candidate_num if is_temp_hum_like(c)]  # temp/hum\n",
    "\n",
    "    # human系：家電human + PIR*\n",
    "    human_cols = [c for c in candidate_num if c.lower().endswith(\"_human\")] + pir_cols\n",
    "\n",
    "    # appliance_only：家電prefixだけ（PIRなし）\n",
    "    appliance_only = [c for c in df.columns if c.startswith(APPLIANCE_PREFIXES)]\n",
    "    appliance_only_co2 = [c for c in appliance_only if is_co2(c)]\n",
    "    appliance_only_pm25 = [c for c in appliance_only if is_pm25(c)]\n",
    "    appliance_only_voc = [c for c in appliance_only if is_voc(c)]\n",
    "    appliance_only_th = [c for c in appliance_only if is_temp_hum_like(c)]\n",
    "    appliance_only_human = [c for c in appliance_only if c.lower().endswith(\"_human\")]\n",
    "\n",
    "    pack = dict(\n",
    "        label=label,\n",
    "        # all\n",
    "        co2_cols=uniq([c for c in co2_cols if c in df.columns]),\n",
    "        pm25_cols=uniq([c for c in pm25_cols if c in df.columns]),\n",
    "        voc_cols=uniq([c for c in voc_cols if c in df.columns]),\n",
    "        th_cols=uniq([c for c in th_cols if c in df.columns]),\n",
    "        human_cols=uniq([c for c in human_cols if c in df.columns]),\n",
    "        # appliance_only\n",
    "        app_co2=uniq([c for c in appliance_only_co2 if c in df.columns]),\n",
    "        app_pm25=uniq([c for c in appliance_only_pm25 if c in df.columns]),\n",
    "        app_voc=uniq([c for c in appliance_only_voc if c in df.columns]),\n",
    "        app_th=uniq([c for c in appliance_only_th if c in df.columns]),\n",
    "        app_human=uniq([c for c in appliance_only_human if c in df.columns]),\n",
    "    )\n",
    "    return pack\n",
    "\n",
    "\n",
    "# =========================\n",
    "# データ拡張（trainのみ）\n",
    "# =========================\n",
    "def augment_train_data(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    noise_level=0.02,\n",
    "    shift_steps=(-1, 1),\n",
    "    repeat_noise=1,\n",
    "    random_state=42,\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # ★念のため：重複カラム除去（上でもやるが二重保険）\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "\n",
    "    X_list = [X]\n",
    "    y_list = [y.reset_index(drop=True)]\n",
    "\n",
    "    # ★「実際に代入する数値列」をここで確定させる（ズレ防止）\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X_num = X[num_cols]  # これが真の形状\n",
    "\n",
    "    # 1) ノイズ付加（repeat分だけ作る）\n",
    "    for k in range(int(repeat_noise)):\n",
    "        if len(num_cols) > 0:\n",
    "            noise = rng.normal(0, noise_level, size=X_num.shape)\n",
    "            Xn = X.copy()\n",
    "            Xn.loc[:, num_cols] = X_num.values + noise\n",
    "            X_list.append(Xn)\n",
    "            y_list.append(y.reset_index(drop=True))\n",
    "\n",
    "    # 2) タイムシフト\n",
    "    for step in shift_steps:\n",
    "        Xs = X.shift(step).ffill().bfill()\n",
    "        X_list.append(Xs)\n",
    "        y_list.append(y.reset_index(drop=True))\n",
    "\n",
    "    X_aug = pd.concat(X_list, axis=0, ignore_index=True)\n",
    "    y_aug = pd.concat(y_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "# =========================\n",
    "# モデル学習（RF / LGBM）\n",
    "# =========================\n",
    "def train_and_eval_models(\n",
    "    room: str, setting: str, df: pd.DataFrame, pack: dict, model_kind: str\n",
    "):\n",
    "    \"\"\"\n",
    "    model_kind: \"rf\" or \"lgbm\"\n",
    "    setting:\n",
    "      - \"all\"\n",
    "      - \"appliance_only\"\n",
    "      - \"no_co2_human\"\n",
    "    \"\"\"\n",
    "    label_col = pack[\"label\"]\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[SKIP] {room}: label {label_col} not found\")\n",
    "        return\n",
    "\n",
    "    # y整形（0/1/2のみ）\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\")\n",
    "    y = y.where(y.isin(ALLOWED_Y), np.nan)\n",
    "    y = y.ffill().fillna(0).astype(int)\n",
    "\n",
    "    # columns choose\n",
    "    if setting == \"all\":\n",
    "        num_cols = (\n",
    "            pack[\"co2_cols\"] + pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        )\n",
    "        human_cols = pack[\"human_cols\"]\n",
    "    elif setting == \"appliance_only\":\n",
    "        num_cols = pack[\"app_co2\"] + pack[\"app_pm25\"] + pack[\"app_voc\"] + pack[\"app_th\"]\n",
    "        human_cols = pack[\"app_human\"]  # PIRなし\n",
    "    elif setting == \"no_co2_human\":\n",
    "        # CO2とhumanを外す：PM2.5/VOC/温湿度等のみ（allベース）\n",
    "        num_cols = pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        human_cols = []  # PIR/humanはゼロ\n",
    "    else:\n",
    "        raise ValueError(\"setting invalid\")\n",
    "\n",
    "    num_cols = uniq([c for c in num_cols if c in df.columns])\n",
    "    human_cols = uniq([c for c in human_cols if c in df.columns])\n",
    "\n",
    "    # 特徴量生成\n",
    "    X_num = (\n",
    "        build_numeric_features_multiwin(df, num_cols, WINS)\n",
    "        if len(num_cols)\n",
    "        else pd.DataFrame(index=df.index)\n",
    "    )\n",
    "    X_hum = (\n",
    "        build_human_features(df, human_cols, WINS)\n",
    "        if len(human_cols)\n",
    "        else pd.DataFrame(index=df.index)\n",
    "    )\n",
    "    X = (\n",
    "        pd.concat([X_num, X_hum], axis=1)\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .ffill()\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "    # 時系列split\n",
    "    tr_idx, te_idx = time_split_idx(len(df), TRAIN_RATIO)\n",
    "    X_tr, X_te = X.iloc[tr_idx].reset_index(drop=True), X.iloc[te_idx].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    y_tr, y_te = y.iloc[tr_idx].reset_index(drop=True), y.iloc[te_idx].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # train拡張（testは絶対拡張しない）\n",
    "    X_tr_aug, y_tr_aug = augment_train_data(\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        noise_level=AUG_NOISE_LEVEL,\n",
    "        shift_steps=AUG_SHIFT_STEPS,\n",
    "        repeat_noise=AUG_REPEAT_NOISE,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # class weight（明示）\n",
    "    sw = compute_sample_weight_from_y(y_tr_aug.values)\n",
    "\n",
    "    # train\n",
    "    if model_kind == \"rf\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=1,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=None,  # sample_weightで調整するため\n",
    "        )\n",
    "        model.fit(X_tr_aug, y_tr_aug, sample_weight=sw)\n",
    "        y_pred = model.predict(X_te)\n",
    "\n",
    "        # importance\n",
    "        importances = model.feature_importances_\n",
    "        imp_title = f\"RF {room} / {setting} | feature importance\"\n",
    "    elif model_kind == \"lgbm\":\n",
    "        # LightGBMログ抑制：verbose=-1\n",
    "        # multi-class（0/1/2）\n",
    "        model = LGBMClassifier(\n",
    "            objective=\"multiclass\",\n",
    "            num_class=3,\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=63,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        model.fit(X_tr_aug, y_tr_aug, sample_weight=sw)\n",
    "        y_pred = model.predict(X_te)\n",
    "\n",
    "        booster = model.booster_\n",
    "        importances = booster.feature_importance(importance_type=\"gain\")\n",
    "        imp_title = f\"LGBM {room} / {setting} | feature importance (gain)\"\n",
    "    else:\n",
    "        raise ValueError(\"model_kind invalid\")\n",
    "\n",
    "    # metrics\n",
    "    acc, bacc, f1m, cm = compact_metrics(y_te, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 92)\n",
    "    print(f\"[{model_kind.upper()}] {room} | setting={setting}\")\n",
    "    print(\n",
    "        f\"features={X.shape[1]}  train={len(X_tr)}(+aug={len(X_tr_aug)})  test={len(X_te)}\"\n",
    "    )\n",
    "    print(f\"label dist test: {y_te.value_counts().to_dict()}\")\n",
    "    print(\"=\" * 92)\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Balanced Acc : {bacc:.4f}\")\n",
    "    print(f\"F1 (macro)   : {f1m:.4f}\")\n",
    "    print(\"Confusion Mat (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "\n",
    "    # save confusion\n",
    "    cm_png = f\"{model_kind}_{room}_{setting}_cm.png\"\n",
    "    plot_confusion_mat(\n",
    "        cm,\n",
    "        labels=[0, 1, 2],\n",
    "        title=f\"{model_kind.upper()} {room} {setting} Confusion\",\n",
    "        save_png=cm_png,\n",
    "    )\n",
    "\n",
    "    # save importance CSV+PNG\n",
    "    tag = f\"{model_kind}_{room}_{setting}\"\n",
    "    save_importance_csv_and_plot(\n",
    "        feature_names=X.columns.tolist(),\n",
    "        importances=importances,\n",
    "        title=imp_title,\n",
    "        tag=tag,\n",
    "        topk=TOPK_IMPORTANCE,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_room(df: pd.DataFrame, room: str):\n",
    "    pack = pick_room_columns(df, room)\n",
    "\n",
    "    # 3条件 × 2モデル（RF/LGBM）\n",
    "    for setting in [\"all\", \"appliance_only\", \"no_co2_human\"]:\n",
    "        train_and_eval_models(room, setting, df, pack, model_kind=\"rf\")\n",
    "        train_and_eval_models(room, setting, df, pack, model_kind=\"lgbm\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "print(\"📥 Loading CSV...\")\n",
    "df0 = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "# timestampが無い場合は「行順」を時系列として扱う（ただし本来はtimestamp推奨）\n",
    "if TIME_COL in df0.columns:\n",
    "    df0[TIME_COL] = to_dt(df0[TIME_COL])\n",
    "    df0 = df0.dropna(subset=[TIME_COL]).sort_values(TIME_COL).set_index(TIME_COL)\n",
    "else:\n",
    "    print(f\"[WARN] {TIME_COL} not found. Using row order as time.\")\n",
    "    df0.index = pd.RangeIndex(len(df0))\n",
    "\n",
    "# 未来を見ない：ffillのみ\n",
    "df0 = safe_ffill(df0)\n",
    "\n",
    "# 1分に揃える（timestampがある場合）\n",
    "if isinstance(df0.index, pd.DatetimeIndex):\n",
    "    df = resample_1min(df0, RESAMPLE_RULE)\n",
    "else:\n",
    "    df = df0.copy()\n",
    "\n",
    "print(\n",
    "    \"\\n================================================================================\"\n",
    ")\n",
    "print(\"RUN ROOM = Living\")\n",
    "print(\n",
    "    \"================================================================================\"\n",
    ")\n",
    "run_room(df, \"Living\")\n",
    "\n",
    "print(\n",
    "    \"\\n================================================================================\"\n",
    ")\n",
    "print(\"RUN ROOM = Japanese\")\n",
    "print(\n",
    "    \"================================================================================\"\n",
    ")\n",
    "run_room(df, \"Japanese\")\n",
    "\n",
    "print(\"\\n✅ Done. (CSVs + PNGs saved in current directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Smart Home Occupancy & Counting Pipeline (Final Optimized)\n",
    "# ============================================================\n",
    "# Features:\n",
    "# 1. Advanced Feature Engineering (Rolling Trend, Slope, Integral)\n",
    "# 2. Time-based Features (Hour Sin/Cos, Weekend)\n",
    "# 3. Model Optimization:\n",
    "#    - Living: RandomForest (GroupShuffleSplit by 30min blocks) + Smoothing\n",
    "#    - Japanese: LightGBM (TimeSplit) + Threshold Tuning\n",
    "# 4. Leakage Prevention (Strict separation)\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "# ここを修正しました\n",
    "CSV_PATH = \"smart_home_data.csv\"\n",
    "TIME_COL = \"timestamp\"\n",
    "RESAMPLE_RULE = \"1min\"\n",
    "\n",
    "# Multi-window sizes for rolling features\n",
    "WINS = [5, 10, 15, 20, 25, 30, 60]\n",
    "TOPK_IMPORTANCE = 20\n",
    "SEED = 42\n",
    "\n",
    "# Living RF Config\n",
    "BLOCK_MINUTES = 30\n",
    "TEST_RATIO = 0.20\n",
    "\n",
    "# Appliance ID Prefixes\n",
    "APPLIANCE_PREFIXES = (\"C0A8033B\", \"C0A80341\", \"C0A80367\", \"C0A80368\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def to_dt(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(series, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def safe_ffill(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.ffill().infer_objects(copy=False)\n",
    "\n",
    "\n",
    "def resample_1min(df: pd.DataFrame, rule=\"1min\") -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    other_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "    out = []\n",
    "    if num_cols:\n",
    "        out.append(df[num_cols].resample(rule).mean())\n",
    "    if other_cols:\n",
    "        out.append(df[other_cols].resample(rule).last())\n",
    "\n",
    "    dfr = pd.concat(out, axis=1).sort_index()\n",
    "    dfr = safe_ffill(dfr)\n",
    "    dfr = dfr.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    return dfr\n",
    "\n",
    "\n",
    "def coerce_boolish_to_float(s: pd.Series) -> pd.Series:\n",
    "    s = s.replace({\"True\": 1, \"False\": 0, True: 1, False: 0})\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s.astype(float)\n",
    "\n",
    "\n",
    "def uniq(xs):\n",
    "    return list(dict.fromkeys(xs))\n",
    "\n",
    "\n",
    "# --- Threshold Optimization ---\n",
    "def optimize_threshold(y_true, y_proba):\n",
    "    best_th = 0.5\n",
    "    best_score = 0\n",
    "    # Search 0.1 to 0.9 for best F1 score of positive class\n",
    "    for th in np.arange(0.1, 0.9, 0.05):\n",
    "        y_pred = (y_proba >= th).astype(int)\n",
    "        score = f1_score(y_true, y_pred, pos_label=1)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_th = th\n",
    "    return best_th\n",
    "\n",
    "\n",
    "# --- Smoothing (Post-Processing) ---\n",
    "def apply_smoothing(y_pred, window=5):\n",
    "    # Rolling median (majority vote for ordinal 0/1/2)\n",
    "    s = pd.Series(y_pred)\n",
    "    return s.rolling(window, center=True, min_periods=1).median().astype(int).values\n",
    "\n",
    "\n",
    "def plot_cm_and_importance(\n",
    "    title_prefix, y_true, y_pred, labels, feature_names, importances, topk=20\n",
    "):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    if hasattr(importances, \"values\"):  # pandas series\n",
    "        imp = importances\n",
    "    else:\n",
    "        imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "    top = imp.head(topk)[::-1]\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # CM\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    im = ax1.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    ax1.set_title(f\"{title_prefix}\\nAccuracy={acc:.4f}\")\n",
    "    plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    tick = np.arange(len(labels))\n",
    "    ax1.set_xticks(tick)\n",
    "    ax1.set_yticks(tick)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.set_yticklabels(labels)\n",
    "    ax1.set_xlabel(\"Predicted\")\n",
    "    ax1.set_ylabel(\"True\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax1.text(\n",
    "                j,\n",
    "                i,\n",
    "                str(cm[i, j]),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\" if cm[i, j] < cm.max() / 2 else \"white\",\n",
    "            )\n",
    "\n",
    "    # Importance\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.barh(top.index, top.values, color=\"skyblue\")\n",
    "    ax2.set_title(f\"Feature Importance (Top{topk})\")\n",
    "    ax2.set_xlabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title_prefix.replace(' ', '_').replace('|', '')}.png\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature Engineering\n",
    "# =========================\n",
    "\n",
    "\n",
    "def _rolling_linreg_slope(y: pd.Series, window: int) -> pd.Series:\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").astype(float)\n",
    "    w = int(window)\n",
    "    if w < 3:\n",
    "        return pd.Series(0.0, index=y.index)\n",
    "    t = np.arange(w, dtype=float)\n",
    "    t_centered = t - t.mean()\n",
    "    denom = np.sum(t_centered**2)\n",
    "\n",
    "    def slope_fn(a):\n",
    "        return np.dot(t_centered, (a - a.mean())) / denom\n",
    "\n",
    "    return y.rolling(w, min_periods=w).apply(slope_fn, raw=True).fillna(0.0)\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        # Cyclical Hour\n",
    "        df[\"hour_sin\"] = np.sin(2 * np.pi * df.index.hour / 24.0)\n",
    "        df[\"hour_cos\"] = np.cos(2 * np.pi * df.index.hour / 24.0)\n",
    "        # Weekend flag\n",
    "        df[\"is_weekend\"] = df.index.dayofweek.isin([5, 6]).astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_numeric_features_multiwin(\n",
    "    df: pd.DataFrame, cols: list[str], wins: list[int]\n",
    ") -> pd.DataFrame:\n",
    "    feats = {}\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\").astype(float).ffill().fillna(0.0)\n",
    "        feats[c] = s\n",
    "\n",
    "        # 1-step diff\n",
    "        d1 = s.diff().fillna(0.0)\n",
    "        feats[f\"{c}_diff_1min\"] = d1\n",
    "\n",
    "        for w in wins:\n",
    "            # Rolling stats\n",
    "            r = s.rolling(w, min_periods=1)\n",
    "            feats[f\"{c}_mean_{w}min\"] = r.mean().fillna(0.0)\n",
    "            feats[f\"{c}_std_{w}min\"] = r.std().fillna(0.0)\n",
    "\n",
    "            # Trends\n",
    "            feats[f\"{c}_slope_{w}min\"] = ((s - s.shift(w)) / float(w)).fillna(0.0)\n",
    "            feats[f\"{c}_trend_{w}min\"] = _rolling_linreg_slope(s, w)\n",
    "\n",
    "    X = pd.DataFrame(feats, index=df.index)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def build_human_features(\n",
    "    df: pd.DataFrame, human_cols: list[str], wins: list[int]\n",
    ") -> pd.DataFrame:\n",
    "    feats = {}\n",
    "    for c in human_cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = coerce_boolish_to_float(df[c]).ffill().fillna(0.0)\n",
    "        feats[c] = s\n",
    "        for w in wins:\n",
    "            feats[f\"{c}_sum_{w}min\"] = s.rolling(w, min_periods=1).sum().fillna(0.0)\n",
    "\n",
    "    X = pd.DataFrame(feats, index=df.index)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).ffill().fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "# --- Column Selection Helper ---\n",
    "def pick_room_columns_latest(df: pd.DataFrame, room: str):\n",
    "    if room == \"Living\":\n",
    "        label = \"Label_Living_Count\"\n",
    "        appliance_cols = [\n",
    "            c\n",
    "            for c in df.columns\n",
    "            if c.startswith(\"C0A80367-013001_\") or c.startswith(\"C0A8033B-013501_\")\n",
    "        ]\n",
    "    elif room == \"Japanese\":\n",
    "        label = \"Label_Japanese_Count\"\n",
    "        appliance_cols = [\n",
    "            c\n",
    "            for c in df.columns\n",
    "            if c.startswith(\"C0A80368-013001_\") or c.startswith(\"C0A80341-013501_\")\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"room must be Living or Japanese\")\n",
    "\n",
    "    m5_cols = [\n",
    "        c\n",
    "        for c in df.columns\n",
    "        if (\n",
    "            c.startswith(\"M5Stack1_\")\n",
    "            or c.startswith(\"M5Stack2_\")\n",
    "            or c.startswith(\"M5Stack8_\")\n",
    "        )\n",
    "    ]\n",
    "    pir_cols = [c for c in df.columns if c.lower().startswith(\"pir\")]\n",
    "    candidate_num = appliance_cols + m5_cols\n",
    "\n",
    "    def check(name, key):\n",
    "        return key in name.lower() or name.lower().endswith(f\"_{key}\")\n",
    "\n",
    "    return dict(\n",
    "        label=label,\n",
    "        co2_cols=uniq([c for c in candidate_num if \"co2\" in c.lower()]),\n",
    "        pm25_cols=uniq([c for c in candidate_num if \"pm2\" in c.lower()]),\n",
    "        voc_cols=uniq([c for c in candidate_num if \"voc\" in c.lower()]),\n",
    "        th_cols=uniq(\n",
    "            [c for c in candidate_num if \"temp\" in c.lower() or \"hum\" in c.lower()]\n",
    "        ),\n",
    "        human_cols=uniq([c for c in candidate_num if \"human\" in c.lower()] + pir_cols),\n",
    "        # Appliance specific subsets\n",
    "        app_co2=uniq([c for c in appliance_cols if \"co2\" in c.lower()]),\n",
    "        app_pm25=uniq([c for c in appliance_cols if \"pm2\" in c.lower()]),\n",
    "        app_voc=uniq([c for c in appliance_cols if \"voc\" in c.lower()]),\n",
    "        app_th=uniq(\n",
    "            [c for c in appliance_cols if \"temp\" in c.lower() or \"hum\" in c.lower()]\n",
    "        ),\n",
    "        app_human=uniq([c for c in appliance_cols if \"human\" in c.lower()]),\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Japanese Room (LightGBM)\n",
    "# =========================\n",
    "def run_japanese_lgbm(df: pd.DataFrame, setting: str = \"all\"):\n",
    "    room = \"Japanese\"\n",
    "    print(f\"\\n>>> Running {room} Pipeline ({setting})\")\n",
    "\n",
    "    pack = pick_room_columns_latest(df, room)\n",
    "    label_col = pack[\"label\"]\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[SKIP] {label_col} not found\")\n",
    "        return\n",
    "\n",
    "    # Prepare Y\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "    y = y.where(y.isin([0, 1, 2]), 0)\n",
    "\n",
    "    # Prepare X\n",
    "    if setting == \"all\":\n",
    "        num_cols = (\n",
    "            pack[\"co2_cols\"] + pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        )\n",
    "        hum_cols = pack[\"human_cols\"]\n",
    "    elif setting == \"appliance_only\":\n",
    "        num_cols = pack[\"app_co2\"] + pack[\"app_pm25\"] + pack[\"app_voc\"] + pack[\"app_th\"]\n",
    "        hum_cols = pack[\"app_human\"]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    X_num = build_numeric_features_multiwin(df, num_cols, WINS)\n",
    "    X_hum = build_human_features(df, hum_cols, WINS)\n",
    "    X = pd.concat([X_num, X_hum], axis=1)\n",
    "\n",
    "    # Add Time Features\n",
    "    X = add_time_features(X)\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "    # Time Split (80/20)\n",
    "    n = len(df)\n",
    "    n_tr = int(n * 0.80)\n",
    "    X_tr, X_te = X.iloc[:n_tr], X.iloc[n_tr:]\n",
    "    y_tr, y_te = y.iloc[:n_tr], y.iloc[n_tr:]\n",
    "\n",
    "    # --- Stage 1: Occupancy (0 vs >0) with Threshold Tuning ---\n",
    "    y_tr_occ = (y_tr > 0).astype(int)\n",
    "    y_te_occ = (y_te > 0).astype(int)\n",
    "\n",
    "    occ_model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=63,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    occ_model.fit(X_tr, y_tr_occ)\n",
    "\n",
    "    # Optimize Threshold\n",
    "    tr_proba = occ_model.predict_proba(X_tr)[:, 1]\n",
    "    te_proba = occ_model.predict_proba(X_te)[:, 1]\n",
    "    best_th = optimize_threshold(y_tr_occ, tr_proba)\n",
    "    print(f\"[Stage 1] Optimized Threshold: {best_th:.2f}\")\n",
    "\n",
    "    pred_occ = (te_proba >= best_th).astype(int)\n",
    "    print(f\"Stage 1 Accuracy: {accuracy_score(y_te_occ, pred_occ):.4f}\")\n",
    "\n",
    "    # --- Stage 2: Count (1 vs 2) ---\n",
    "    tr_mask = y_tr.isin([1, 2])\n",
    "    X_tr_12 = X_tr[tr_mask]\n",
    "    y_tr_12 = y_tr[tr_mask]\n",
    "\n",
    "    if len(np.unique(y_tr_12)) < 2:\n",
    "        print(\"[WARN] Stage 2 skipped (insufficient data)\")\n",
    "        final_pred = np.zeros_like(y_te)\n",
    "    else:\n",
    "        # Map 1->0, 2->1 for binary classifier\n",
    "        y_tr_map = y_tr_12.map({1: 0, 2: 1})\n",
    "\n",
    "        cnt_model = LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=63,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        cnt_model.fit(X_tr_12, y_tr_map)\n",
    "\n",
    "        # Predict on all test samples\n",
    "        pred_cnt_raw = cnt_model.predict(X_te)  # returns 0 or 1\n",
    "        pred_cnt_mapped = pd.Series(pred_cnt_raw).map({0: 1, 1: 2}).values\n",
    "\n",
    "        # Combine with Stage 1 result\n",
    "        final_pred = np.zeros_like(y_te)\n",
    "        occ_mask = pred_occ == 1\n",
    "        final_pred[occ_mask] = pred_cnt_mapped[occ_mask]\n",
    "\n",
    "    # --- Post-Processing: Smoothing ---\n",
    "    final_pred_smooth = apply_smoothing(final_pred, window=5)\n",
    "\n",
    "    print(\"\\n--- Final Results (Smoothed) ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_te, final_pred_smooth):.4f}\")\n",
    "    print(classification_report(y_te, final_pred_smooth, digits=3, zero_division=0))\n",
    "\n",
    "    plot_cm_and_importance(\n",
    "        f\"Japanese LGBM {setting} Final\",\n",
    "        y_te,\n",
    "        final_pred_smooth,\n",
    "        labels=[0, 1, 2],\n",
    "        feature_names=X.columns.tolist(),\n",
    "        importances=occ_model.feature_importances_,  # Show Stage 1 importance\n",
    "        topk=TOPK_IMPORTANCE,\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Living Room (RandomForest)\n",
    "# =========================\n",
    "def make_block_groups(ts: pd.Series, block_minutes: int) -> np.ndarray:\n",
    "    t0 = ts.min()\n",
    "    minutes = (ts - t0).dt.total_seconds() / 60.0\n",
    "    return (minutes // block_minutes).astype(int).to_numpy()\n",
    "\n",
    "\n",
    "def run_living_rf(df: pd.DataFrame, setting: str = \"all\"):\n",
    "    room = \"Living\"\n",
    "    print(f\"\\n>>> Running {room} Pipeline ({setting})\")\n",
    "\n",
    "    pack = pick_room_columns_latest(df, room)\n",
    "    label_col = pack[\"label\"]\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[SKIP] {label_col} not found\")\n",
    "        return\n",
    "\n",
    "    # Prepare Y\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "    y = y.where(y.isin([0, 1, 2]), 0)\n",
    "\n",
    "    # Prepare X\n",
    "    if setting == \"all\":\n",
    "        num_cols = (\n",
    "            pack[\"co2_cols\"] + pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        )\n",
    "        hum_cols = pack[\"human_cols\"]\n",
    "    elif setting == \"appliance_only\":\n",
    "        num_cols = pack[\"app_co2\"] + pack[\"app_pm25\"] + pack[\"app_voc\"] + pack[\"app_th\"]\n",
    "        hum_cols = pack[\"app_human\"]\n",
    "    elif setting == \"no_co2_human\":\n",
    "        num_cols = pack[\"pm25_cols\"] + pack[\"voc_cols\"] + pack[\"th_cols\"]\n",
    "        hum_cols = []\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    X_num = build_numeric_features_multiwin(df, num_cols, WINS)\n",
    "    X_hum = build_human_features(df, hum_cols, WINS)\n",
    "    X = pd.concat([X_num, X_hum], axis=1)\n",
    "\n",
    "    # Add Time Features\n",
    "    X = add_time_features(X)\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "    # Block Split\n",
    "    groups = make_block_groups(pd.Series(df.index), BLOCK_MINUTES)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=TEST_RATIO, random_state=SEED)\n",
    "    tr_idx, te_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    # --- Stage 1: Occupancy (0 vs >0) ---\n",
    "    y_tr_occ = (y_tr > 0).astype(int)\n",
    "    y_te_occ = (y_te > 0).astype(int)\n",
    "\n",
    "    occ_model = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        min_samples_leaf=2,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    occ_model.fit(X_tr, y_tr_occ)\n",
    "\n",
    "    # Threshold Tuning\n",
    "    tr_proba = occ_model.predict_proba(X_tr)[:, 1]\n",
    "    te_proba = occ_model.predict_proba(X_te)[:, 1]\n",
    "    best_th = optimize_threshold(y_tr_occ, tr_proba)\n",
    "    print(f\"[Stage 1] Optimized Threshold: {best_th:.2f}\")\n",
    "\n",
    "    pred_occ = (te_proba >= best_th).astype(int)\n",
    "    print(f\"Stage 1 Accuracy: {accuracy_score(y_te_occ, pred_occ):.4f}\")\n",
    "\n",
    "    # --- Stage 2: Count (1 vs 2) ---\n",
    "    tr_mask = y_tr.isin([1, 2])\n",
    "    X_tr_12 = X_tr[tr_mask]\n",
    "    y_tr_12 = y_tr[tr_mask]\n",
    "\n",
    "    if len(np.unique(y_tr_12)) < 2:\n",
    "        print(\"[WARN] Stage 2 skipped\")\n",
    "        final_pred = np.zeros_like(y_te)\n",
    "    else:\n",
    "        cnt_model = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            min_samples_leaf=2,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        cnt_model.fit(X_tr_12, y_tr_12)\n",
    "\n",
    "        pred_cnt_all = cnt_model.predict(X_te)\n",
    "\n",
    "        # Combine\n",
    "        final_pred = np.zeros_like(y_te)\n",
    "        occ_mask = pred_occ == 1\n",
    "        final_pred[occ_mask] = pred_cnt_all[occ_mask]\n",
    "\n",
    "    # --- Smoothing ---\n",
    "    final_pred_smooth = apply_smoothing(final_pred, window=5)\n",
    "\n",
    "    print(\"\\n--- Final Results (Smoothed) ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_te, final_pred_smooth):.4f}\")\n",
    "    print(classification_report(y_te, final_pred_smooth, digits=3, zero_division=0))\n",
    "\n",
    "    plot_cm_and_importance(\n",
    "        f\"Living RF {setting} Final\",\n",
    "        y_te,\n",
    "        final_pred_smooth,\n",
    "        labels=[0, 1, 2],\n",
    "        feature_names=X.columns.tolist(),\n",
    "        importances=occ_model.feature_importances_,\n",
    "        topk=TOPK_IMPORTANCE,\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main Execution\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"📥 Loading CSV...\")\n",
    "    try:\n",
    "        # utf-8がデフォルトですが、エラーハンドリング付きで読み込むことも可能です\n",
    "        df0 = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {CSV_PATH} not found.\")\n",
    "        exit()\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Encoding error. Trying 'cp932'...\")\n",
    "        df0 = pd.read_csv(CSV_PATH, low_memory=False, encoding=\"cp932\")\n",
    "\n",
    "    if TIME_COL not in df0.columns:\n",
    "        print(f\"Error: {TIME_COL} missing.\")\n",
    "        exit()\n",
    "\n",
    "    df0[TIME_COL] = to_dt(df0[TIME_COL])\n",
    "    df0 = df0.dropna(subset=[TIME_COL]).sort_values(TIME_COL).set_index(TIME_COL)\n",
    "\n",
    "    # Preprocessing\n",
    "    df = resample_1min(df0, RESAMPLE_RULE)\n",
    "\n",
    "    # Run Pipelines\n",
    "    # 1. Living (RF) - Try different settings\n",
    "    for setting in [\"all\", \"appliance_only\"]:\n",
    "        run_living_rf(df, setting)\n",
    "\n",
    "    # 2. Japanese (LightGBM)\n",
    "    for setting in [\"all\", \"appliance_only\"]:\n",
    "        run_japanese_lgbm(df, setting)\n",
    "\n",
    "    print(\"\\n✅ All Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
